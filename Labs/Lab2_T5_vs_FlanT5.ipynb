{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTmafB83jLYk"
      },
      "source": [
        "# Laboratorio: Comparaci√≥n T5 vs FLAN-T5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io0BTPOejLYl"
      },
      "source": [
        "## SECCI√ìN 1: Setup y Carga de Datos\n",
        "\n",
        "Esta secci√≥n est√° completa. Solo ejecuta las celdas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZL0F3FAjLYm"
      },
      "outputs": [],
      "source": [
        "# Instalaci√≥n de dependencias\n",
        "!pip install transformers datasets evaluate accelerate -q\n",
        "!pip install torch -q\n",
        "!pip install scikit-learn -q\n",
        "\n",
        "# Imports necesarios\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"‚úì Dependencias cargadas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlwxbM2GjLYm"
      },
      "outputs": [],
      "source": [
        "# Cargar datasets con subsets fijos (5000 train, 1000 validation cada uno)\n",
        "print(\"Cargando datasets...\\n\")\n",
        "\n",
        "# Dataset 1: SST-2 (Sentimiento de pel√≠culas)\n",
        "print(\"  - SST-2 (Sentimiento de pel√≠culas)...\")\n",
        "dataset_sst2 = load_dataset(\"glue\", \"sst2\")\n",
        "dataset_sst2_train = dataset_sst2[\"train\"].select(range(5000))\n",
        "dataset_sst2_val = dataset_sst2[\"validation\"].select(range(872)) # No tiene mas ejemplos\n",
        "\n",
        "# Dataset 2: Amazon Polarity (Reviews de productos)\n",
        "print(\"  - Amazon Polarity (Reviews de productos)...\")\n",
        "dataset_amazon = load_dataset(\"amazon_polarity\")\n",
        "dataset_amazon_train = dataset_amazon[\"train\"].select(range(5000))\n",
        "dataset_amazon_val = dataset_amazon[\"test\"].select(range(1000))\n",
        "\n",
        "# Dataset 3: AG News (Clasificaci√≥n de noticias en 4 categor√≠as)\n",
        "print(\"  - AG News (Noticias en 4 categor√≠as)...\")\n",
        "dataset_agnews = load_dataset(\"ag_news\")\n",
        "dataset_agnews_train = dataset_agnews[\"train\"].select(range(5000))\n",
        "dataset_agnews_val = dataset_agnews[\"test\"].select(range(1000))\n",
        "\n",
        "print(\"\\n‚úì Datasets cargados\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaHgs-YFjLYn"
      },
      "outputs": [],
      "source": [
        "# Exploraci√≥n de los datasets\n",
        "print(\"--- SST-2 (Sentimiento de pel√≠culas) ---\")\n",
        "print(f\"Train: {len(dataset_sst2_train)} ejemplos\")\n",
        "print(f\"Validation: {len(dataset_sst2_val)} ejemplos\")\n",
        "print(f\"Clases: 0=negative, 1=positive\")\n",
        "print(f\"Ejemplo: {dataset_sst2_train[0]}\")\n",
        "\n",
        "print(\"\\n--- Amazon Polarity (Reviews de productos) ---\")\n",
        "print(f\"Train: {len(dataset_amazon_train)} ejemplos\")\n",
        "print(f\"Validation: {len(dataset_amazon_val)} ejemplos\")\n",
        "print(f\"Clases: 0=negative, 1=positive\")\n",
        "print(f\"Ejemplo: {dataset_amazon_train[0]}\")\n",
        "\n",
        "print(\"\\n--- AG News (Noticias) ---\")\n",
        "print(f\"Train: {len(dataset_agnews_train)} ejemplos\")\n",
        "print(f\"Validation: {len(dataset_agnews_val)} ejemplos\")\n",
        "print(f\"Clases: 0=World, 1=Sports, 2=Business, 3=Sci/Tech\")\n",
        "print(f\"Ejemplo: {dataset_agnews_train[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7S6ZgjWjLYn"
      },
      "outputs": [],
      "source": [
        "# Cargar tokenizador\n",
        "model_checkpoint = \"t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "print(\"‚úì Tokenizador T5-base cargado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCYU7MQdjLYn"
      },
      "source": [
        "## SECCI√ìN 2: Preprocesamiento de Datos\n",
        "\n",
        "En esta secci√≥n ver√°s un ejemplo completo (SST-2) y tendr√°s que adaptar el c√≥digo para los otros dos datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooCl2JxIjLYn"
      },
      "source": [
        "### Ejemplo Completo: SST-2\n",
        "\n",
        "Usa este c√≥digo como referencia para completar los TODOs siguientes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytdkh3fkjLYo"
      },
      "outputs": [],
      "source": [
        "def preprocess_function_sst2(examples):\n",
        "    \"\"\"\n",
        "    Preprocesa el dataset SST-2 para T5.\n",
        "    Convierte la tarea de clasificaci√≥n a formato text-to-text.\n",
        "    \"\"\"\n",
        "    # Prefijo que indica la tarea\n",
        "    prefix = \"sst2 sentence: \"\n",
        "\n",
        "    # Mapeo de etiquetas num√©ricas a texto\n",
        "    label_map = {0: \"negative\", 1: \"positive\"}\n",
        "\n",
        "    # A√±adir prefijo a cada oraci√≥n\n",
        "    inputs = [prefix + doc for doc in examples[\"sentence\"]]\n",
        "\n",
        "    # Tokenizar inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)\n",
        "\n",
        "    # Convertir labels a texto\n",
        "    labels_text = [label_map[label] for label in examples[\"label\"]]\n",
        "\n",
        "    # Tokenizar labels\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(labels_text, max_length=2, truncation=True, padding=False)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Aplicar preprocesamiento\n",
        "tokenized_sst2_train = dataset_sst2_train.map(\n",
        "    preprocess_function_sst2,\n",
        "    batched=True,\n",
        "    remove_columns=dataset_sst2_train.column_names\n",
        ")\n",
        "tokenized_sst2_val = dataset_sst2_val.map(\n",
        "    preprocess_function_sst2,\n",
        "    batched=True,\n",
        "    remove_columns=dataset_sst2_val.column_names\n",
        ")\n",
        "\n",
        "print(\"‚úì SST-2 preprocesado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30gHyqRvjLYo"
      },
      "source": [
        "### TODO 1: Preprocesar Amazon Polarity\n",
        "\n",
        "**Pistas:**\n",
        "- El dataset usa `'content'` y `'title'` en vez de `'sentence'`\n",
        "- Puedes concatenar: `title + ' ' + content`\n",
        "- El prefijo debe ser: `'amazon review: '`\n",
        "- Las labels son iguales: `0=negative, 1=positive`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14ghfVdWjLYo"
      },
      "outputs": [],
      "source": [
        "def preprocess_function_amazon(examples):\n",
        "    \"\"\"\n",
        "    TODO: Completa esta funci√≥n bas√°ndote en el ejemplo de SST-2\n",
        "    \"\"\"\n",
        "    # TODO: Define el prefijo\n",
        "    prefix = \"amazon review: \"\n",
        "\n",
        "    # TODO: Define el mapeo de labels (igual que SST-2)\n",
        "    label_map = {0: \"negative\", 1: \"positive\"}\n",
        "\n",
        "    # TODO: Concatena title + content y a√±ade prefijo\n",
        "    # Hint: inputs = [prefix + title + \" \" + content for title, content in zip(...)]\n",
        "    inputs = # TU C√ìDIGO AQU√ç\n",
        "\n",
        "    # TODO: Tokeniza los inputs (copia de SST-2)\n",
        "    model_inputs = # TU C√ìDIGO AQU√ç\n",
        "\n",
        "    # TODO: Convierte labels a texto\n",
        "    labels_text = # TU C√ìDIGO AQU√ç\n",
        "\n",
        "    # TODO: Tokeniza las labels\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = # TU C√ìDIGO AQU√ç\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# TODO: Aplica el preprocesamiento (copia la estructura de SST-2)\n",
        "tokenized_amazon_train = # TU C√ìDIGO AQU√ç\n",
        "tokenized_amazon_val = # TU C√ìDIGO AQU√ç\n",
        "\n",
        "print(\"‚úì Amazon Polarity preprocesado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXWfl5kHjLYo"
      },
      "source": [
        "### TODO 2: Preprocesar AG News\n",
        "\n",
        "**Pistas:**\n",
        "- El dataset usa `'text'` para el contenido\n",
        "- El prefijo debe ser: `'ag news: '`\n",
        "- Ahora son 4 clases: `0=World, 1=Sports, 2=Business, 3=Sci/Tech`\n",
        "- `max_length` para labels debe ser 3 (palabras m√°s largas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHH6hdPbjLYo"
      },
      "outputs": [],
      "source": [
        "def preprocess_function_agnews(examples):\n",
        "    \"\"\"\n",
        "    TODO: Completa esta funci√≥n para AG News (4 clases)\n",
        "    \"\"\"\n",
        "    # TODO: Define el prefijo\n",
        "    prefix = # TU C√ìDIGO AQU√ç\n",
        "\n",
        "    # TODO: Define el mapeo de labels (4 clases ahora)\n",
        "    label_map = {\n",
        "        0: # TU C√ìDIGO AQU√ç,\n",
        "        1: # TU C√ìDIGO AQU√ç,\n",
        "        2: # TU C√ìDIGO AQU√ç,\n",
        "        3: # TU C√ìDIGO AQU√ç\n",
        "    }\n",
        "\n",
        "    # TODO: A√±ade prefijo al texto\n",
        "    inputs = # TU C√ìDIGO AQU√ç\n",
        "\n",
        "    # TODO: Tokeniza los inputs\n",
        "    model_inputs = # TU C√ìDIGO AQU√ç\n",
        "\n",
        "    # TODO: Convierte labels a texto\n",
        "    labels_text = # TU C√ìDIGO AQU√ç\n",
        "\n",
        "    # TODO: Tokeniza las labels (max_length=3 porque las palabras son m√°s largas)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = # TU C√ìDIGO AQU√ç\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# TODO: Aplica el preprocesamiento\n",
        "tokenized_agnews_train = # TU C√ìDIGO AQU√ç\n",
        "tokenized_agnews_val = # TU C√ìDIGO AQU√ç\n",
        "\n",
        "print(\"‚úì AG News preprocesado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgQ70Ev_jLYp"
      },
      "source": [
        "## SECCI√ìN 3: Fine-tuning de T5-base\n",
        "\n",
        "Esta secci√≥n est√° completa. El c√≥digo entrenar√° T5-base en los 3 datasets autom√°ticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrk1kne1jLYp"
      },
      "outputs": [],
      "source": [
        "# M√©tricas\n",
        "metric_accuracy = evaluate.load(\"accuracy\")\n",
        "metric_f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Calcula accuracy y F1 score para las predicciones.\n",
        "    CORRECCI√ìN: Compara strings directamente sin usar la librer√≠a evaluate para accuracy\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Decodificar predicciones\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Decodificar labels - reemplazar -100 con pad_token_id\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # CORRECCI√ìN: Calcular accuracy manualmente comparando strings\n",
        "    # Normalizar strings (strip y lowercase)\n",
        "    decoded_preds_normalized = [pred.strip().lower() for pred in decoded_preds]\n",
        "    decoded_labels_normalized = [label.strip().lower() for label in decoded_labels]\n",
        "\n",
        "    # Calcular accuracy manualmente\n",
        "    correct = sum(p == l for p, l in zip(decoded_preds_normalized, decoded_labels_normalized))\n",
        "    accuracy = correct / len(decoded_labels_normalized)\n",
        "\n",
        "    # Para F1: convertir strings a IDs num√©ricos\n",
        "    unique_labels = sorted(list(set(decoded_labels_normalized)))\n",
        "    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "    pred_ids = [label_to_id.get(pred, -1) for pred in decoded_preds_normalized]\n",
        "    label_ids = [label_to_id.get(label, -1) for label in decoded_labels_normalized]\n",
        "\n",
        "    # Filtrar predicciones inv√°lidas (que no matchean ninguna label conocida)\n",
        "    valid_indices = [i for i, pred_id in enumerate(pred_ids) if pred_id != -1]\n",
        "\n",
        "    if len(valid_indices) > 0:\n",
        "        pred_ids_valid = [pred_ids[i] for i in valid_indices]\n",
        "        label_ids_valid = [label_ids[i] for i in valid_indices]\n",
        "\n",
        "        f1 = metric_f1.compute(\n",
        "            predictions=pred_ids_valid,\n",
        "            references=label_ids_valid,\n",
        "            average='macro'\n",
        "        )\n",
        "    else:\n",
        "        f1 = {\"f1\": 0.0}\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1[\"f1\"]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKx48I0pjLYp"
      },
      "outputs": [],
      "source": [
        "def train_t5_on_dataset(train_dataset, val_dataset, output_dir, dataset_name):\n",
        "    \"\"\"\n",
        "    Entrena T5-base en un dataset espec√≠fico.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüöÄ Entrenando T5-base en {dataset_name}...\")\n",
        "\n",
        "    # Cargar modelo fresco\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "    # Configurar argumentos de entrenamiento\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        eval_strategy=\"epoch\",\n",
        "        learning_rate=3e-4,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        weight_decay=0.01,\n",
        "        save_total_limit=1,\n",
        "        num_train_epochs=1,  # Solo 1 √©poca para velocidad\n",
        "        predict_with_generate=True,\n",
        "        report_to=\"none\",\n",
        "        fp16=False,\n",
        "        logging_steps=100,\n",
        "    )\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=-100\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Entrenar\n",
        "    start_time = datetime.now()\n",
        "    trainer.train()\n",
        "    end_time = datetime.now()\n",
        "\n",
        "    # Evaluar\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    print(f\"‚úì Entrenamiento completado en {(end_time - start_time).seconds}s\")\n",
        "    print(f\"  Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "    print(f\"  F1 Score: {eval_results['eval_f1']:.4f}\")\n",
        "\n",
        "    return trainer, eval_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Zp_mI6RjLYp"
      },
      "outputs": [],
      "source": [
        "# Entrenar en los 3 datasets (esto tomar√° ~15-20 minutos)\n",
        "print(\"Entrenando T5-base en los 3 datasets...\\n\")\n",
        "\n",
        "trainer_sst2, results_sst2 = train_t5_on_dataset(\n",
        "    tokenized_sst2_train,\n",
        "    tokenized_sst2_val,\n",
        "    \"t5-sst2-finetuned\",\n",
        "    \"SST-2\"\n",
        ")\n",
        "\n",
        "trainer_amazon, results_amazon = train_t5_on_dataset(\n",
        "    tokenized_amazon_train,\n",
        "    tokenized_amazon_val,\n",
        "    \"t5-amazon-finetuned\",\n",
        "    \"Amazon Polarity\"\n",
        ")\n",
        "\n",
        "trainer_agnews, results_agnews = train_t5_on_dataset(\n",
        "    tokenized_agnews_train,\n",
        "    tokenized_agnews_val,\n",
        "    \"t5-agnews-finetuned\",\n",
        "    \"AG News\"\n",
        ")\n",
        "\n",
        "print(\"\\n‚úì Fine-tuning de T5-base completo para los 3 datasets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCxe2ljEjLYq"
      },
      "source": [
        "## SECCI√ìN 4: Evaluaci√≥n FLAN-T5 Zero-Shot\n",
        "\n",
        "Ahora dise√±ar√°s prompts para FLAN-T5 y ver√°s c√≥mo se desempe√±a sin entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoHzvp2EjLYq"
      },
      "outputs": [],
      "source": [
        "# Cargar FLAN-T5\n",
        "print(\"Cargando FLAN-T5...\")\n",
        "model_flan = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "tokenizer_flan = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "print(\"‚úì FLAN-T5 cargado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T4sCIbqjLYq"
      },
      "outputs": [],
      "source": [
        "def evaluate_flan_t5(dataset, prompt_template, label_map):\n",
        "    \"\"\"\n",
        "    Eval√∫a FLAN-T5 en zero-shot sobre un dataset.\n",
        "    CORRECCI√ìN: Calcula accuracy manualmente sin usar metric_accuracy.compute()\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    print(f\"  Evaluando {len(dataset)} ejemplos...\")\n",
        "\n",
        "    for i, example in enumerate(dataset):\n",
        "        if i % 200 == 0:\n",
        "            print(f\"    Progreso: {i}/{len(dataset)}\")\n",
        "\n",
        "        # Generar prompt\n",
        "        prompt = prompt_template(example)\n",
        "\n",
        "        # Generar predicci√≥n\n",
        "        input_ids = tokenizer_flan(prompt, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
        "        outputs = model_flan.generate(input_ids, max_length=10)\n",
        "        prediction = tokenizer_flan.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        predictions.append(prediction)\n",
        "        references.append(label_map[example[\"label\"]])\n",
        "\n",
        "    # Normalizar strings (strip y lowercase)\n",
        "    predictions_normalized = [pred.strip().lower() for pred in predictions]\n",
        "    references_normalized = [ref.strip().lower() for ref in references]\n",
        "\n",
        "    # Calcular accuracy manualmente\n",
        "    correct = sum(p == r for p, r in zip(predictions_normalized, references_normalized))\n",
        "    accuracy = correct / len(references_normalized)\n",
        "\n",
        "    # F1 score - convertir strings a IDs num√©ricos\n",
        "    unique_labels = sorted(list(set(references_normalized)))\n",
        "    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "    pred_ids = [label_to_id.get(pred, -1) for pred in predictions_normalized]\n",
        "    ref_ids = [label_to_id.get(ref, -1) for ref in references_normalized]\n",
        "\n",
        "    # Filtrar predicciones inv√°lidas\n",
        "    valid_indices = [i for i, pred_id in enumerate(pred_ids) if pred_id != -1]\n",
        "\n",
        "    if len(valid_indices) > 0:\n",
        "        pred_ids_valid = [pred_ids[i] for i in valid_indices]\n",
        "        ref_ids_valid = [ref_ids[i] for i in valid_indices]\n",
        "\n",
        "        f1 = metric_f1.compute(\n",
        "            predictions=pred_ids_valid,\n",
        "            references=ref_ids_valid,\n",
        "            average='macro'\n",
        "        )\n",
        "    else:\n",
        "        f1 = {\"f1\": 0.0}\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1[\"f1\"]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFOol_sYjLYq"
      },
      "source": [
        "### TODO 3: Dise√±a prompts efectivos para FLAN-T5\n",
        "\n",
        "Experimenta con diferentes formulaciones y observa cu√°l funciona mejor.\n",
        "\n",
        "**Caracter√≠sticas de un buen prompt:**\n",
        "- Instrucci√≥n clara y directa\n",
        "- Especifica las opciones exactas de respuesta\n",
        "- Contexto m√≠nimo pero suficiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LnDreLQjLYq"
      },
      "outputs": [],
      "source": [
        "# TODO: Dise√±a un prompt para SST-2\n",
        "def prompt_sst2(example):\n",
        "    \"\"\"\n",
        "    TODO: Dise√±a un prompt claro para clasificaci√≥n de sentimiento.\n",
        "    El prompt debe pedirle a FLAN-T5 que responda 'positive' o 'negative'.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"# TU C√ìDIGO AQU√ç\n",
        "    # Ejemplo: \"Classify the sentiment of this review as positive or negative: {example['sentence']}\"\n",
        "    \"\"\"\n",
        "    return prompt\n",
        "\n",
        "# TODO: Dise√±a un prompt para Amazon\n",
        "def prompt_amazon(example):\n",
        "    \"\"\"\n",
        "    TODO: Dise√±a un prompt para reviews de productos Amazon.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"# TU C√ìDIGO AQU√ç\n",
        "    \"\"\"\n",
        "    return prompt\n",
        "\n",
        "# TODO: Dise√±a un prompt para AG News\n",
        "def prompt_agnews(example):\n",
        "    \"\"\"\n",
        "    TODO: Dise√±a un prompt para clasificaci√≥n de noticias en 4 categor√≠as.\n",
        "    Las categor√≠as son: World, Sports, Business, Sci/Tech\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"# TU C√ìDIGO AQU√ç\n",
        "    \"\"\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVejl4uxjLYq"
      },
      "outputs": [],
      "source": [
        "# Mapeos de labels\n",
        "label_map_sst2 = {0: \"negative\", 1: \"positive\"}\n",
        "label_map_amazon = {0: \"negative\", 1: \"positive\"}\n",
        "label_map_agnews = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMvTuqsijLYr"
      },
      "outputs": [],
      "source": [
        "# Evaluar FLAN-T5 en los 3 datasets\n",
        "print(\"\\nEvaluando FLAN-T5 en SST-2...\")\n",
        "results_flan_sst2 = evaluate_flan_t5(dataset_sst2_val, prompt_sst2, label_map_sst2)\n",
        "print(f\"‚úì SST-2 - Accuracy: {results_flan_sst2['accuracy']:.4f}, F1: {results_flan_sst2['f1']:.4f}\")\n",
        "\n",
        "print(\"\\nEvaluando FLAN-T5 en Amazon...\")\n",
        "results_flan_amazon = evaluate_flan_t5(dataset_amazon_val, prompt_amazon, label_map_amazon)\n",
        "print(f\"‚úì Amazon - Accuracy: {results_flan_amazon['accuracy']:.4f}, F1: {results_flan_amazon['f1']:.4f}\")\n",
        "\n",
        "print(\"\\nEvaluando FLAN-T5 en AG News...\")\n",
        "results_flan_agnews = evaluate_flan_t5(dataset_agnews_val, prompt_agnews, label_map_agnews)\n",
        "print(f\"‚úì AG News - Accuracy: {results_flan_agnews['accuracy']:.4f}, F1: {results_flan_agnews['f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBDhkgMkjLYr"
      },
      "source": [
        "## SECCI√ìN 5: Comparaci√≥n y An√°lisis\n",
        "\n",
        "Ahora compararemos los resultados y discutiremos las implicaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fBAN-IljLYr"
      },
      "outputs": [],
      "source": [
        "# Tabla comparativa - Accuracy\n",
        "print(\"\\nüìä TABLA COMPARATIVA - ACCURACY\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Dataset':<20} | {'T5 Fine-tuned':<15} | {'FLAN-T5 Zero-shot':<18} | {'Diferencia':<12}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "diff_sst2 = results_sst2['eval_accuracy'] - results_flan_sst2['accuracy']\n",
        "print(f\"{'SST-2':<20} | {results_sst2['eval_accuracy']:>14.2%} | {results_flan_sst2['accuracy']:>17.2%} | {diff_sst2:>+11.2%}\")\n",
        "\n",
        "diff_amazon = results_amazon['eval_accuracy'] - results_flan_amazon['accuracy']\n",
        "print(f\"{'Amazon Polarity':<20} | {results_amazon['eval_accuracy']:>14.2%} | {results_flan_amazon['accuracy']:>17.2%} | {diff_amazon:>+11.2%}\")\n",
        "\n",
        "diff_agnews = results_agnews['eval_accuracy'] - results_flan_agnews['accuracy']\n",
        "print(f\"{'AG News':<20} | {results_agnews['eval_accuracy']:>14.2%} | {results_flan_agnews['accuracy']:>17.2%} | {diff_agnews:>+11.2%}\")\n",
        "\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrmGOuhGjLYr"
      },
      "outputs": [],
      "source": [
        "# Tabla comparativa - F1 Score\n",
        "print(\"\\nüìä TABLA COMPARATIVA - F1 SCORE (MACRO)\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Dataset':<20} | {'T5 Fine-tuned':<15} | {'FLAN-T5 Zero-shot':<18} | {'Diferencia':<12}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "diff_f1_sst2 = results_sst2['eval_f1'] - results_flan_sst2['f1']\n",
        "print(f\"{'SST-2':<20} | {results_sst2['eval_f1']:>14.2%} | {results_flan_sst2['f1']:>17.2%} | {diff_f1_sst2:>+11.2%}\")\n",
        "\n",
        "diff_f1_amazon = results_amazon['eval_f1'] - results_flan_amazon['f1']\n",
        "print(f\"{'Amazon Polarity':<20} | {results_amazon['eval_f1']:>14.2%} | {results_flan_amazon['f1']:>17.2%} | {diff_f1_amazon:>+11.2%}\")\n",
        "\n",
        "diff_f1_agnews = results_agnews['eval_f1'] - results_flan_agnews['f1']\n",
        "print(f\"{'AG News':<20} | {results_agnews['eval_f1']:>14.2%} | {results_flan_agnews['f1']:>17.2%} | {diff_f1_agnews:>+11.2%}\")\n",
        "\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctiyf94FjLYr"
      },
      "source": [
        "## Preguntas de Reflexi√≥n\n",
        "\n",
        "Discute las siguientes preguntas con tu grupo:\n",
        "\n",
        "1. **¬øEn qu√© dataset FLAN-T5 se acerca m√°s al rendimiento de T5 fine-tuned?**\n",
        "   ¬øPor qu√© crees que esto ocurre?\n",
        "\n",
        "2. **¬øPor qu√© crees que AG News (4 clases) podr√≠a ser m√°s dif√≠cil para FLAN-T5 en zero-shot comparado con los datasets binarios?**\n",
        "\n",
        "3. **Observa las diferencias de accuracy. ¬øCu√°ndo vale la pena hacer fine-tuning en vez de usar FLAN-T5 zero-shot en producci√≥n?**\n",
        "\n",
        "4. **¬øC√≥mo afecta la calidad del prompt al rendimiento de FLAN-T5?**\n",
        "   Experimenta modificando los prompts y observa si mejoran los resultados.\n",
        "\n",
        "5. **Si tuvieras que deployar un sistema de clasificaci√≥n de texto en producci√≥n:**\n",
        "   - ¬øCon datos etiquetados disponibles? ‚Üí ¬øQu√© usar√≠as?\n",
        "   - ¬øSin datos etiquetados? ‚Üí ¬øQu√© usar√≠as?\n",
        "   - ¬øPara m√∫ltiples tareas diferentes? ‚Üí ¬øQu√© usar√≠as?\n",
        "\n",
        "6. **BONUS: ¬øQu√© pasar√≠a si usaras few-shot prompting con FLAN-T5?**\n",
        "   (A√±adir 2-3 ejemplos en el prompt antes de la predicci√≥n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOBXnDK0jLYr"
      },
      "source": [
        "## ‚úì Laboratorio Completado\n",
        "\n",
        "### Resumen de aprendizajes clave:\n",
        "\n",
        "1. **T5-base** es un modelo poderoso que requiere fine-tuning para tareas espec√≠ficas\n",
        "2. **FLAN-T5** demuestra el poder del instruction tuning para generalizaci√≥n zero-shot\n",
        "3. La **calidad del prompt** es crucial para el rendimiento de FLAN-T5\n",
        "4. **Fine-tuning vs Zero-shot** es una decisi√≥n que depende del contexto y recursos disponibles\n",
        "5. Entender estos trade-offs es esencial para deployment en producci√≥n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
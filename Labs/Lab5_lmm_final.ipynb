{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b6db13",
   "metadata": {},
   "source": [
    "# Lab: Large Multimodal Models\n",
    "**Diplomado en Inteligencia Artificial - Universidad de Chile**\n",
    "\n",
    "**Duraci√≥n:** 1.5 horas  \n",
    "**Modelos:** SmolVLM-500M-Instruct + BLIP-2-OPT-2.7B  \n",
    "**GPU:** T4 (Google Colab)\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos del Lab\n",
    "1. Explorar capacidades de modelos multimodales modernos\n",
    "2. Experimentar con prompting y Visual Question Answering (VQA)\n",
    "3. Identificar limitaciones sistem√°ticas de los LMMs\n",
    "4. Comparar arquitecturas distintas (SmolVLM 2024 vs BLIP-2 2023)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be197db",
   "metadata": {},
   "source": [
    "## Setup: Instalaci√≥n y configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dddd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Instalar dependencias necesarias\"\"\"\n",
    "!pip install -q transformers accelerate pillow torch requests open-clip-torch einops-exts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8619d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imports generales\"\"\"\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from IPython.display import display\n",
    "\n",
    "# Verificar GPU disponible\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146af17",
   "metadata": {},
   "source": [
    "## Im√°genes del Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab845d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Diccionario de im√°genes para el lab\"\"\"\n",
    "IMAGES = {\n",
    "    # Warm-up: Captioning b√°sico\n",
    "    \"dog_park\": \"https://irondoggy.com/cdn/shop/articles/dog-running-in-a-dog-park_1258x.jpg?v=1706177184\",\n",
    "    \"mountain_lake\": \"https://storage.googleapis.com/chile-travel-cdn/2021/03/puerto-octay-1024x540-4.jpeg\",\n",
    "    \"cooking_kitchen\": \"https://img.freepik.com/foto-gratis/17-estilos-vida-personas-que-piden-sushi-domicilio_52683-100626.jpg?semt=ais_incoming&w=740&q=80\",\n",
    "    \n",
    "    # Prompting dirigido\n",
    "    \"urban_scene\": \"https://dynamic-media-cdn.tripadvisor.com/media/photo-o/03/4f/aa/d8/paseo-ahumada.jpg?w=900&h=500&s=1\",\n",
    "    \n",
    "    # VQA\n",
    "    \"family_picnic\": \"https://wallpapers.com/images/hd/the-office-season-8-picnic-y30cb9o08up4apf6.jpg\",\n",
    "    \"modern_office\": \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTwkDaHQHdS80lL7I0b2vqvoz2nXnuKf5vmWw&s\",\n",
    "    \n",
    "    # L√≠mites: Conteo\n",
    "    \"fruit_bowl\": \"https://magpiestyle.co.nz/cdn/shop/files/IMG_4250_raguwx.jpg?v=1732496459&width=1149\",\n",
    "    \n",
    "    # L√≠mites: Texto\n",
    "    \"street_sign\": \"https://ecopsa.com/wp-content/uploads/2015/08/senalizacion-ecopsa2.jpg\",\n",
    "    \n",
    "    # L√≠mites: Espacial\n",
    "    \"living_room\": \"https://anticostudio.com/cdn/shop/files/fgf.jpg?v=1749473042\",\n",
    "    \n",
    "    # L√≠mites: Detalles\n",
    "    \"colorful_objects\": \"https://dynamic-media-cdn.tripadvisor.com/media/photo-o/16/1f/25/0b/img-20190116-wa0010-01.jpg?w=500&h=500&s=1\",\n",
    "    \n",
    "    # Memes (opcional)\n",
    "    \"blursed_1\": \"https://preview.redd.it/blursed-captcha-v0-chc7r4fokz1g1.jpeg?width=1080&crop=smart&auto=webp&s=7fff0b090cfc4cd429252fdcb8f7a27f39b8ce68\",\n",
    "    \"blursed_2\": \"https://preview.redd.it/blursed-bread-v0-eh4bviemlz1g1.jpeg?width=640&crop=smart&auto=webp&s=bf8748fcc5ed8dcf4c68b84bb64a6ebe60034171\",\n",
    "    \"text_meme\": \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS81kLrsNo0aM2miN4Kwys3Mhxqpk8PZHTGGg&s\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de58b40a",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d84565",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"Funciones para cargar im√°genes e inferencia\"\"\"\n",
    "\n",
    "def load_image(url):\n",
    "    \"\"\"Cargar imagen desde URL\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        return Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando imagen: {e}\")\n",
    "        return None\n",
    "\n",
    "def infer_caption_smolvlm(model, processor, image, prompt=\"\", max_tokens=100):\n",
    "    \"\"\"Generar texto desde SmolVLM (requiere token <image>)\"\"\"\n",
    "    if image is None:\n",
    "        return \"Error: imagen no v√°lida\"\n",
    "    \n",
    "    # SmolVLM requiere formato especial con <image> token\n",
    "    if prompt and \"<image>\" not in prompt:\n",
    "        full_prompt = f\"<image>{prompt}\"\n",
    "    elif not prompt:\n",
    "        full_prompt = \"<image>Describe this image.\"\n",
    "    else:\n",
    "        full_prompt = prompt\n",
    "    \n",
    "    inputs = processor(images=image, text=full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "    \n",
    "    return processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "def infer_caption_blip2(model, processor, image, prompt=\"\", max_tokens=100):\n",
    "    \"\"\"Generar texto desde BLIP-2 (NO requiere token <image>)\"\"\"\n",
    "    if image is None:\n",
    "        return \"Error: imagen no v√°lida\"\n",
    "    \n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "    \n",
    "    return processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "def show_result(image, prompt, result):\n",
    "    \"\"\"Mostrar imagen y resultado\"\"\"\n",
    "    print(f\"üìù Prompt: {prompt}\")\n",
    "    display(image.resize((400, 300)) if image else None)\n",
    "    print(f\"ü§ñ Respuesta: {result}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ddbb07",
   "metadata": {},
   "source": [
    "---\n",
    "# PARTE 1: SmolVLM-500M-Instruct\n",
    "Modelo moderno y ligero (~2GB VRAM, 2024)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee90bcc",
   "metadata": {},
   "source": [
    "## Cargar modelo SmolVLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd586cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cargar SmolVLM-500M-Instruct\"\"\"\n",
    "print(\"Cargando SmolVLM-500M-Instruct...\")\n",
    "\n",
    "MODEL_NAME_1 = \"HuggingFaceTB/SmolVLM-500M-Instruct\"\n",
    "processor_1 = AutoProcessor.from_pretrained(MODEL_NAME_1, trust_remote_code=True)\n",
    "model_1 = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_NAME_1,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Modelo cargado en: {model_1.device}\")\n",
    "print(f\"üìä Par√°metros: ~500M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f2496b",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Warm-up: Captioning b√°sico\n",
    "\n",
    "Generaci√≥n autom√°tica de descripciones sin instrucciones espec√≠ficas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Captioning b√°sico: 3 im√°genes simples\"\"\"\n",
    "\n",
    "warmup_images = [\"dog_park\", \"mountain_lake\", \"cooking_kitchen\"]\n",
    "\n",
    "print(\"üî• WARM-UP: Captioning b√°sico\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for img_key in warmup_images:\n",
    "    url = IMAGES[img_key]\n",
    "    if not url:\n",
    "        print(f\"‚ö†Ô∏è  Falta URL para: {img_key}\")\n",
    "        continue\n",
    "    \n",
    "    image = load_image(url)\n",
    "    if image:\n",
    "        result = infer_caption_smolvlm(model_1, processor_1, image, prompt=\"Describe this image.\")\n",
    "        show_result(image, \"Describe this image.\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef32ac",
   "metadata": {},
   "source": [
    "### üí≠ Reflexi√≥n\n",
    "- ¬øLas descripciones capturan lo esencial de cada imagen?\n",
    "- ¬øQu√© detalles menciona y cu√°les omite?\n",
    "- ¬øHay alucinaciones (menciona cosas que no est√°n)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af7f15a",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Prompting dirigido\n",
    "\n",
    "Misma imagen, distintos prompts ‚Üí distintas respuestas.\n",
    "\n",
    "**Tu tarea:** Completa la lista de prompts con tus propias instrucciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prompting dirigido: experimentar con distintas instrucciones\"\"\"\n",
    "\n",
    "# COMPLETAR: Escribe 3 prompts distintos\n",
    "prompts = [\n",
    "    \"Describe this image in one sentence.\",  # Ejemplo base\n",
    "    \"recreate a history based on this image\",  # TU PROMPT 1\n",
    "    \"tell me whats wrong in this image\",  # TU PROMPT 2\n",
    "]\n",
    "\n",
    "# Cargar imagen\n",
    "url = IMAGES[\"urban_scene\"]\n",
    "if url:\n",
    "    image = load_image(url)\n",
    "    \n",
    "    if image:\n",
    "        print(\"üéØ PROMPTING DIRIGIDO: Misma imagen, distintos prompts\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, prompt in enumerate(prompts, 1):\n",
    "            if not prompt:\n",
    "                print(f\"‚ö†Ô∏è  Prompt {i} vac√≠o - compl√©talo arriba\\n\")\n",
    "                continue\n",
    "            \n",
    "            result = infer_caption_smolvlm(model_1, processor_1, image, prompt=prompt)\n",
    "            show_result(image, prompt, result)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Falta URL para 'urban_scene'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30bdb1",
   "metadata": {},
   "source": [
    "### üí≠ Reflexi√≥n\n",
    "- ¬øC√≥mo cambia la respuesta seg√∫n el prompt?\n",
    "- ¬øQu√© tipo de prompt genera mejores resultados?\n",
    "- ¬øAlg√∫n prompt confundi√≥ al modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a000a",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Visual Question Answering (VQA)\n",
    "\n",
    "Hacer preguntas espec√≠ficas sobre im√°genes.\n",
    "\n",
    "**Tipos de preguntas:**\n",
    "- **Factual:** Hechos verificables (ej: colores, cantidad)\n",
    "- **Inferencial:** Requiere interpretaci√≥n (ej: emociones, intenciones)\n",
    "- **Razonamiento:** Requiere l√≥gica (ej: por qu√©, qu√© pas√≥ antes/despu√©s)\n",
    "\n",
    "**Tu tarea:** Completa las listas de preguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949fe233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VQA: Preguntas sobre familia en picnic\"\"\"\n",
    "\n",
    "# COMPLETAR: Escribe 3 preguntas (una de cada tipo)\n",
    "questions_picnic = [\n",
    "    \"How many people are in this image?\",  # Ejemplo: factual\n",
    "    \"what emotion are they feeling\",  # TU PREGUNTA INFERENCIAL\n",
    "    \"why might they be outdoors\",  # TU PREGUNTA DE RAZONAMIENTO\n",
    "]\n",
    "\n",
    "url = IMAGES[\"family_picnic\"]\n",
    "if url:\n",
    "    image = load_image(url)\n",
    "    \n",
    "    if image:\n",
    "        print(\"‚ùì VQA: Familia en picnic\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for q in questions_picnic:\n",
    "            if not q:\n",
    "                print(\"‚ö†Ô∏è  Pregunta vac√≠a - compl√©tala arriba\\n\")\n",
    "                continue\n",
    "            \n",
    "            result = infer_caption_smolvlm(model_1, processor_1, image, prompt=q)\n",
    "            show_result(image, q, result)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Falta URL para 'family_picnic'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284bf9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VQA: Preguntas sobre oficina moderna\"\"\"\n",
    "\n",
    "# COMPLETAR: Escribe 3 preguntas (una de cada tipo)\n",
    "questions_office = [\n",
    "    \"What technology can you see in this image?\",  # Ejemplo: factual\n",
    "    \"what type of service do they provide\",  # TU PREGUNTA INFERENCIAL\n",
    "    \"how much do they make based on the resources displayed in the image?\",  # TU PREGUNTA DE RAZONAMIENTO\n",
    "]\n",
    "\n",
    "url = IMAGES[\"modern_office\"]\n",
    "if url:\n",
    "    image = load_image(url)\n",
    "    \n",
    "    if image:\n",
    "        print(\"‚ùì VQA: Oficina moderna\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for q in questions_office:\n",
    "            if not q:\n",
    "                print(\"‚ö†Ô∏è  Pregunta vac√≠a - compl√©tala arriba\\n\")\n",
    "                continue\n",
    "            \n",
    "            result = infer_caption_smolvlm(model_1, processor_1, image, prompt=q)\n",
    "            show_result(image, q, result)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Falta URL para 'modern_office'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c4fac",
   "metadata": {},
   "source": [
    "### üí≠ Reflexi√≥n\n",
    "- ¬øEl modelo responde correctamente las preguntas factuales?\n",
    "- ¬øPuede hacer inferencias razonables?\n",
    "- ¬øD√≥nde es m√°s d√©bil: hechos, inferencias, o razonamiento?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c047c4b",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Encontrando los l√≠mites\n",
    "\n",
    "Probar sistem√°ticamente d√≥nde fallan los modelos multimodales.\n",
    "\n",
    "### a) Conteo de objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"L√≠mite 1: Conteo de objetos\"\"\"\n",
    "\n",
    "url = IMAGES[\"fruit_bowl\"]\n",
    "if url:\n",
    "    image = load_image(url)\n",
    "    \n",
    "    if image:\n",
    "        print(\"üî¢ L√çMITE: Conteo de objetos\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Preguntas de conteo\n",
    "        count_questions = [\n",
    "            \"How many fruits are in this image?\",\n",
    "            \"Count all the objects you can see.\",\n",
    "            \"How many red objects are there?\"\n",
    "        ]\n",
    "        \n",
    "        for q in count_questions:\n",
    "            result = infer_caption_smolvlm(model_1, processor_1, image, prompt=q)\n",
    "            show_result(image, q, result)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Falta URL para 'fruit_bowl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e18466",
   "metadata": {},
   "source": [
    "**Observaci√≥n:** Los LMMs generalmente son malos contando. ¬øAcert√≥ en este caso?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b4cf1",
   "metadata": {},
   "source": [
    "### b) Texto en im√°genes (lectura OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ddb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"L√≠mite 2: Leer texto en im√°genes\"\"\"\n",
    "\n",
    "url = IMAGES[\"street_sign\"]\n",
    "if url:\n",
    "    image = load_image(url)\n",
    "    \n",
    "    if image:\n",
    "        print(\"üìù L√çMITE: Texto en im√°genes\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        text_questions = [\n",
    "            \"What does the sign say?\",\n",
    "            \"Read the text in this image.\",\n",
    "            \"What words can you see?\"\n",
    "        ]\n",
    "        \n",
    "        for q in text_questions:\n",
    "            result = infer_caption_smolvlm(model_1, processor_1, image, prompt=q)\n",
    "            show_result(image, q, result)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Falta URL para 'street_sign'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424fe33",
   "metadata": {},
   "source": [
    "**Observaci√≥n:** ¬øPuede leer correctamente? ¬øSolo parcialmente? ¬øAlucina texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597ac34",
   "metadata": {},
   "source": [
    "### c) Razonamiento espacial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"L√≠mite 3: Razonamiento espacial (posiciones relativas)\"\"\"\n",
    "\n",
    "url = IMAGES[\"living_room\"]\n",
    "if url:\n",
    "    image = load_image(url)\n",
    "    \n",
    "    if image:\n",
    "        print(\"üìê L√çMITE: Razonamiento espacial\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        spatial_questions = [\n",
    "            \"What is to the left of the sofa?\",\n",
    "            \"Describe the position of objects in the room.\",\n",
    "            \"What is in the center of the image?\"\n",
    "        ]\n",
    "        \n",
    "        for q in spatial_questions:\n",
    "            result = infer_caption_smolvlm(model_1, processor_1, image, prompt=q)\n",
    "            show_result(image, q, result)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Falta URL para 'living_room'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8836d0",
   "metadata": {},
   "source": [
    "**Observaci√≥n:** ¬øEntiende correctamente izquierda/derecha, arriba/abajo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddab0a3",
   "metadata": {},
   "source": [
    "### d) Detalles finos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02645052",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"L√≠mite 4: Detalles finos (colores, tama√±os, texturas)\"\"\"\n",
    "\n",
    "url = IMAGES[\"colorful_objects\"]\n",
    "if url:\n",
    "    image = load_image(url)\n",
    "    \n",
    "    if image:\n",
    "        print(\"üîç L√çMITE: Detalles finos\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        detail_questions = [\n",
    "            \"What color is the smallest object?\",\n",
    "            \"Describe the textures you can see.\",\n",
    "            \"What is the brightest colored item?\"\n",
    "        ]\n",
    "        \n",
    "        for q in detail_questions:\n",
    "            result = infer_caption_smolvlm(model_1, processor_1, image, prompt=q)\n",
    "            show_result(image, q, result)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Falta URL para 'colorful_objects'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b860bb6",
   "metadata": {},
   "source": [
    "**Observaci√≥n:** ¬øNota detalles peque√±os o solo lo m√°s obvio?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c93fe9",
   "metadata": {},
   "source": [
    "### üí≠ Resumen de l√≠mites de SmolVLM\n",
    "\n",
    "Escribe 3-4 oraciones resumiendo:\n",
    "- ¬øEn qu√© tareas el modelo es bueno?\n",
    "- ¬øD√≥nde falla consistentemente?\n",
    "- ¬øQu√© tipo de errores comete (alucinaciones, imprecisi√≥n, etc)?\n",
    "\n",
    "**Tu an√°lisis:**\n",
    "\n",
    "*(Escribe aqu√≠)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7993189",
   "metadata": {},
   "source": [
    "---\n",
    "# PARTE 2: BLIP-2-OPT-2.7B\n",
    "Modelo de 2023, arquitectura cl√°sica (~2.7B par√°metros, ~5GB VRAM)\n",
    "\n",
    "Vamos a repetir algunos experimentos clave para comparar con SmolVLM.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea9965c",
   "metadata": {},
   "source": [
    "## Liberar memoria y cargar BLIP-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cbefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Liberar VRAM del modelo anterior\"\"\"\n",
    "print(\"üßπ Liberando memoria...\")\n",
    "\n",
    "del model_1, processor_1\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"‚úÖ Memoria liberada. VRAM disponible: ~{torch.cuda.mem_get_info()[0] / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cargar BLIP-2-OPT-2.7B\"\"\"\n",
    "print(\"Cargando BLIP-2-OPT-2.7B...\")\n",
    "\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "MODEL_NAME_2 = \"Salesforce/blip2-opt-2.7b\"\n",
    "processor_2 = Blip2Processor.from_pretrained(MODEL_NAME_2)\n",
    "model_2 = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME_2,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Modelo cargado en: {model_2.device}\")\n",
    "print(f\"üìä Par√°metros: ~2.7B\")\n",
    "print(f\"üìÖ A√±o: 2023 (arquitectura cl√°sica)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143812d2",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparaci√≥n 1: Prompting dirigido\n",
    "\n",
    "Repetimos el experimento de prompting con la misma imagen y prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Comparaci√≥n: Prompting con BLIP-2\"\"\"\n",
    "\n",
    "# Usar los mismos prompts que antes\n",
    "prompts_comparison = [\n",
    "    \"Describe this image in one sentence.\",\n",
    "    \"List all the objects and people you can see.\",\n",
    "    \"What is the overall mood or atmosphere of this scene?\"\n",
    "]\n",
    "\n",
    "url = IMAGES[\"urban_scene\"]\n",
    "if url:\n",
    "    image = load_image(url)\n",
    "    \n",
    "    if image:\n",
    "        print(\"üîÑ COMPARACI√ìN: Prompting (BLIP-2)\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for prompt in prompts_comparison:\n",
    "            result = infer_caption_blip2(model_2, processor_2, image, prompt=prompt)\n",
    "            show_result(image, prompt, result)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Falta URL para 'urban_scene'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da14460",
   "metadata": {},
   "source": [
    "### üí≠ Comparaci√≥n SmolVLM vs BLIP-2 (Prompting)\n",
    "\n",
    "- ¬øCu√°l dio descripciones m√°s detalladas?\n",
    "- ¬øCu√°l fue m√°s preciso?\n",
    "- ¬øNotaste diferencias en el estilo de respuesta?\n",
    "- ¬øEl modelo m√°s nuevo (SmolVLM 2024) es realmente mejor que BLIP-2 (2023)?\n",
    "\n",
    "**Tu an√°lisis:**\n",
    "\n",
    "*(Escribe aqu√≠)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad136ef",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparaci√≥n 2: L√≠mites (Conteo + Texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410858fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Comparaci√≥n: Conteo con BLIP-2\"\"\"\n",
    "\n",
    "url = IMAGES[\"fruit_bowl\"]\n",
    "if url:\n",
    "    image = load_image(url)\n",
    "    \n",
    "    if image:\n",
    "        print(\"üî¢ COMPARACI√ìN: Conteo (BLIP-2)\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        q = \"How many fruits are in this image?\"\n",
    "        result = infer_caption_blip2(model_2, processor_2, image, prompt=q)\n",
    "        show_result(image, q, result)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Falta URL para 'fruit_bowl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Comparaci√≥n: Texto con BLIP-2\"\"\"\n",
    "\n",
    "url = IMAGES[\"street_sign\"]\n",
    "if url:\n",
    "    image = load_image(url)\n",
    "    \n",
    "    if image:\n",
    "        print(\"üìù COMPARACI√ìN: Texto (BLIP-2)\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        q = \"What does the sign say?\"\n",
    "        result = infer_caption_blip2(model_2, processor_2, image, prompt=q)\n",
    "        show_result(image, q, result)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Falta URL para 'street_sign'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a38cb7",
   "metadata": {},
   "source": [
    "### üí≠ Comparaci√≥n SmolVLM vs BLIP-2 (L√≠mites)\n",
    "\n",
    "- ¬øBLIP-2 es mejor en conteo?\n",
    "- ¬øBLIP-2 lee mejor el texto?\n",
    "- ¬øVale la pena usar el modelo m√°s nuevo (SmolVLM) o el cl√°sico (BLIP-2) es suficiente?\n",
    "- ¬øQu√© limitaciones comparten ambos modelos?\n",
    "\n",
    "**Tu an√°lisis:**\n",
    "\n",
    "*(Escribe aqu√≠)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e9492",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus: Memes y bromas con BLIP-2\n",
    "\n",
    "Probemos los l√≠mites del modelo con im√°genes \"raras\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Bonus: Explicar memes/im√°genes confusas con BLIP-2\"\"\"\n",
    "\n",
    "meme_keys = [\"blursed_1\", \"text_meme\"]\n",
    "\n",
    "print(\"üé≠ BONUS: Memes y contenido confuso (BLIP-2)\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for meme_key in meme_keys:\n",
    "    url = IMAGES.get(meme_key)\n",
    "    if not url:\n",
    "        print(f\"‚ö†Ô∏è  Falta URL para: {meme_key}\")\n",
    "        continue\n",
    "    \n",
    "    image = load_image(url)\n",
    "    if image:\n",
    "        # Intentar explicar qu√© es raro/gracioso\n",
    "        prompts_meme = [\n",
    "            \"Describe what you see in this image.\",\n",
    "            \"What is unusual or funny about this image?\",\n",
    "            \"Explain why this might be considered humorous.\"\n",
    "        ]\n",
    "        \n",
    "        for p in prompts_meme:\n",
    "            result = infer_caption_blip2(model_2, processor_2, image, prompt=p)\n",
    "            show_result(image, p, result)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbedabe",
   "metadata": {},
   "source": [
    "### üí≠ Reflexi√≥n sobre memes\n",
    "\n",
    "- ¬øBLIP-2 capt√≥ el humor o la rareza?\n",
    "- ¬øQu√© tipo de conocimiento necesitar√≠a para \"entender\" la broma?\n",
    "- ¬øEs esto una limitaci√≥n fundamental o solo cuesti√≥n de entrenamiento?\n",
    "\n",
    "**Tu an√°lisis:**\n",
    "\n",
    "*(Escribe aqu√≠)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe8391",
   "metadata": {},
   "source": [
    "---\n",
    "# PARTE 3: Playground libre con SmolVLM\n",
    "\n",
    "Volvemos a cargar SmolVLM para experimentar libremente.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283078d",
   "metadata": {},
   "source": [
    "## Re-cargar SmolVLM para playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11442a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Liberar VRAM de BLIP-2\"\"\"\n",
    "print(\"üßπ Liberando memoria de BLIP-2...\")\n",
    "\n",
    "del model_2, processor_2\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"‚úÖ Memoria liberada. VRAM disponible: ~{torch.cuda.mem_get_info()[0] / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf62d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Re-cargar SmolVLM-500M-Instruct para playground\"\"\"\n",
    "print(\"Recargando SmolVLM-500M-Instruct para playground...\")\n",
    "\n",
    "processor_playground = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-500M-Instruct\", trust_remote_code=True)\n",
    "model_playground = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolVLM-500M-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ SmolVLM recargado para experimentaci√≥n libre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146954b",
   "metadata": {},
   "source": [
    "## Funci√≥n para probar im√°genes propias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5c2a4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"Funci√≥n para probar im√°genes propias\"\"\"\n",
    "\n",
    "def probar_imagen(url, prompt=\"Describe this image in detail.\"):\n",
    "    \"\"\"\n",
    "    Prueba el modelo con tu propia imagen.\n",
    "    \n",
    "    Args:\n",
    "        url: URL de la imagen (Unsplash, Reddit, etc)\n",
    "        prompt: Pregunta o instrucci√≥n para el modelo\n",
    "    \n",
    "    Returns:\n",
    "        None (muestra imagen y resultado)\n",
    "    \"\"\"\n",
    "    image = load_image(url)\n",
    "    if image:\n",
    "        result = infer_caption_smolvlm(model_playground, processor_playground, image, prompt=prompt, max_tokens=150)\n",
    "        show_result(image, prompt, result)\n",
    "    else:\n",
    "        print(\"‚ùå Error cargando imagen. Verifica la URL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5212e15c",
   "metadata": {},
   "source": [
    "## Experimenta aqu√≠\n",
    "\n",
    "Prueba con tus propias im√°genes. Algunas ideas:\n",
    "- Foto personal\n",
    "- Imagen de tu ciudad\n",
    "- Algo espec√≠fico de tu trabajo/investigaci√≥n\n",
    "- Imagen t√©cnica (diagrama, gr√°fico, etc)\n",
    "- Memes o im√°genes raras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66582e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"EXPERIMENTA AQU√ç\"\"\"\n",
    "\n",
    "# Ejemplo 1: Tu imagen\n",
    "probar_imagen(\n",
    "    url=\"\",  # TU URL AQU√ç\n",
    "    prompt=\"Describe this image.\"  # TU PROMPT AQU√ç\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f5f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 2: Otra imagen\n",
    "probar_imagen(\n",
    "    url=\"\",  # TU URL AQU√ç\n",
    "    prompt=\"\"  # TU PROMPT AQU√ç\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce0ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 3: M√°s experimentos\n",
    "probar_imagen(\n",
    "    url=\"\",  # TU URL AQU√ç\n",
    "    prompt=\"\"  # TU PROMPT AQU√ç\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc92962",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusiones del Lab\n",
    "\n",
    "## Resumen de aprendizajes:\n",
    "\n",
    "1. **Capacidades b√°sicas:**\n",
    "   - *(¬øQu√© hacen bien los LMMs en general?)*\n",
    "\n",
    "2. **Limitaciones identificadas:**\n",
    "   - *(¬øD√≥nde fallan sistem√°ticamente? Conteo, texto, espacial, etc)*\n",
    "\n",
    "3. **Diferencias entre modelos:**\n",
    "   - *(SmolVLM 2024 vs BLIP-2 2023: ¬øha mejorado realmente la tecnolog√≠a?)*\n",
    "   - *(¬øCu√°ndo vale la pena usar un modelo m√°s grande/nuevo?)*\n",
    "\n",
    "4. **Implicaciones pr√°cticas:**\n",
    "   - *(¬øPara qu√© casos de uso s√≠/no usar√≠as estos modelos?)*\n",
    "   - *(¬øQu√© tipo de aplicaciones son viables hoy?)*\n",
    "\n",
    "**Tus conclusiones:**\n",
    "\n",
    "*(Escribe aqu√≠)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfbcdec",
   "metadata": {},
   "source": [
    "---\n",
    "## Recursos adicionales\n",
    "\n",
    "### Modelos:\n",
    "- **SmolVLM:** https://huggingface.co/HuggingFaceTB/SmolVLM-500M-Instruct\n",
    "- **BLIP-2:** https://huggingface.co/Salesforce/blip2-opt-2.7b\n",
    "- **Otros LMMs:**\n",
    "  - LLaVA: https://huggingface.co/llava-hf\n",
    "  - Qwen2-VL: https://huggingface.co/Qwen\n",
    "  - PaliGemma: https://huggingface.co/google/paligemma\n",
    "\n",
    "### Papers:\n",
    "- **BLIP-2:** https://arxiv.org/abs/2301.12597\n",
    "- **CLIP:** https://arxiv.org/abs/2103.00020\n",
    "- **Flamingo:** https://arxiv.org/abs/2204.14198\n",
    "- **LLaVA:** https://arxiv.org/abs/2304.08485\n",
    "\n",
    "### Datasets de evaluaci√≥n:\n",
    "- **VQAv2:** https://visualqa.org/\n",
    "- **COCO Captions:** https://cocodataset.org/\n",
    "- **TextVQA:** https://textvqa.org/\n",
    "- **GQA:** https://cs.stanford.edu/people/dorarad/gqa/\n",
    "\n",
    "### Tutoriales:\n",
    "- **Hugging Face Vision:** https://huggingface.co/docs/transformers/tasks/image_captioning\n",
    "- **OpenAI Vision Guide:** https://platform.openai.com/docs/guides/vision\n",
    "\n",
    "---\n",
    "**Fin del Lab**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f723ef",
   "metadata": {},
   "source": [
    "# Hands-on Session 6: Audio Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e737a4f",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this hands-on session, you will be able to:\n",
    "- Understand how audio is represented digitally (sampling, amplitude, bit depth)\n",
    "- Load and explore audio datasets using ü§ó Datasets\n",
    "- Visualize audio data in different representations (waveform, spectrogram, mel spectrogram)\n",
    "- Process and preprocess audio data using librosa\n",
    "- Prepare audio data for machine learning models\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python knowledge\n",
    "- Understanding of NumPy arrays\n",
    "- Familiarity with matplotlib for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c2fee5",
   "metadata": {},
   "source": [
    "## Setup: Install Required Libraries\n",
    "\n",
    "First, let's install the necessary libraries for working with audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae1a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install -q torchcodec==0.7 datasets[audio] librosa matplotlib transformers soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperFeatureExtractor, AutoProcessor\n",
    "import IPython.display as ipd\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba198618",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Understanding Audio Data Representation\n",
    "\n",
    "## 1.1 From Continuous to Digital: Sampling\n",
    "\n",
    "Audio is a **continuous signal** in the physical world - sound waves are continuous changes in air pressure. However, computers can only work with discrete, finite values. To convert continuous audio into digital form, we use **sampling**.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "**Sampling** is the process of measuring the value of a continuous signal at fixed time intervals.\n",
    "\n",
    "**Sampling Rate (or Sampling Frequency)** is the number of samples taken per second, measured in Hertz (Hz).\n",
    "\n",
    "Common sampling rates:\n",
    "- **16 kHz (16,000 Hz)**: Common for speech recognition models\n",
    "- **22.05 kHz**: Used for lower-quality audio\n",
    "- **44.1 kHz**: CD-quality audio\n",
    "- **48 kHz**: Professional audio/video\n",
    "- **192 kHz**: High-resolution audio\n",
    "\n",
    "**Important**: The sampling rate determines the highest frequency that can be captured, known as the **Nyquist limit** (= sampling_rate / 2).\n",
    "\n",
    "For example:\n",
    "- Speech sampled at 16 kHz can capture frequencies up to 8 kHz (sufficient for human speech)\n",
    "- Music typically needs 44.1 kHz to capture frequencies up to ~20 kHz (human hearing range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4a9b15",
   "metadata": {},
   "source": [
    "## 1.2 Amplitude and Bit Depth\n",
    "\n",
    "**Amplitude** describes the sound pressure level (loudness) at any given instant, measured in decibels (dB).\n",
    "\n",
    "**Bit Depth** determines the precision with which amplitude values are recorded:\n",
    "- **16-bit**: 65,536 possible amplitude levels (standard for most audio)\n",
    "- **24-bit**: 16,777,216 possible amplitude levels (professional audio)\n",
    "- **32-bit float**: Used in ML (values normalized to [-1.0, 1.0] range)\n",
    "\n",
    "For machine learning, audio is typically converted to 32-bit floating-point format with values in the range [-1.0, 1.0]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86017d87",
   "metadata": {},
   "source": [
    "## 1.3 Hands-on: Loading and Exploring Audio with Librosa\n",
    "\n",
    "Let's start by loading a sample audio file using librosa. Librosa comes with several example audio files we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8760708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example audio file (trumpet sound)\n",
    "audio_array, sampling_rate = librosa.load(librosa.ex('trumpet'))\n",
    "\n",
    "print(f\"Audio shape: {audio_array.shape}\")\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"Duration: {len(audio_array) / sampling_rate:.2f} seconds\")\n",
    "print(f\"Data type: {audio_array.dtype}\")\n",
    "print(f\"Value range: [{audio_array.min():.3f}, {audio_array.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4485f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to the audio\n",
    "ipd.Audio(audio_array, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0067b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Audio Representations and Visualizations\n",
    "\n",
    "## 2.1 Time Domain: Waveform\n",
    "\n",
    "The **waveform** is the most intuitive representation - it shows amplitude over time (time domain representation).\n",
    "\n",
    "Useful for:\n",
    "- Identifying timing of sound events\n",
    "- Overall loudness assessment\n",
    "- Detecting noise or irregularities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf98e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the waveform\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(audio_array, sr=sampling_rate)\n",
    "plt.title('Waveform (Time Domain Representation)')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8cf767",
   "metadata": {},
   "source": [
    "## 2.2 Frequency Domain: Spectrum\n",
    "\n",
    "The **frequency spectrum** shows which frequencies are present in the signal and their strength (frequency domain representation).\n",
    "\n",
    "It's calculated using the **Discrete Fourier Transform (DFT)**, specifically the Fast Fourier Transform (FFT) algorithm.\n",
    "\n",
    "Key points:\n",
    "- X-axis: Frequency (Hz) - typically on log scale\n",
    "- Y-axis: Amplitude (dB)\n",
    "- Shows the frequency composition at a single point in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ef236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the frequency spectrum of the first 4096 samples\n",
    "n_fft = 4096\n",
    "dft_input = audio_array[:n_fft]\n",
    "\n",
    "# Apply windowing to reduce spectral leakage\n",
    "window = np.hanning(len(dft_input))\n",
    "windowed_input = dft_input * window\n",
    "\n",
    "# Calculate DFT\n",
    "dft = np.fft.rfft(windowed_input)\n",
    "\n",
    "# Get amplitude spectrum in decibels\n",
    "amplitude = np.abs(dft)\n",
    "amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)\n",
    "\n",
    "# Get frequency bins\n",
    "frequency = librosa.fft_frequencies(sr=sampling_rate, n_fft=len(dft_input))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(frequency, amplitude_db)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Amplitude (dB)')\n",
    "plt.title('Frequency Spectrum (Frequency Domain Representation)')\n",
    "plt.xscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeba682",
   "metadata": {},
   "source": [
    "## 2.3 Time-Frequency Domain: Spectrogram\n",
    "\n",
    "The **spectrogram** combines time and frequency - it shows how frequencies change over time!\n",
    "\n",
    "It's created by:\n",
    "1. Splitting audio into short overlapping segments (frames)\n",
    "2. Computing FFT for each segment\n",
    "3. Stacking the results together\n",
    "\n",
    "Key properties:\n",
    "- X-axis: Time\n",
    "- Y-axis: Frequency (Hz)\n",
    "- Color/Intensity: Amplitude/Power (dB)\n",
    "\n",
    "Algorithm: **STFT (Short-Time Fourier Transform)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ae42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize the spectrogram\n",
    "D = librosa.stft(audio_array)\n",
    "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(S_db, sr=sampling_rate, x_axis='time', y_axis='hz')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram (Time-Frequency Representation)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Spectrogram shape: {S_db.shape}\")\n",
    "print(f\"(frequency bins, time frames) = ({S_db.shape[0]}, {S_db.shape[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622a57c",
   "metadata": {},
   "source": [
    "## 2.4 Perceptual Representation: Mel Spectrogram\n",
    "\n",
    "The **mel spectrogram** is a variant that mimics human hearing!\n",
    "\n",
    "**Key insight**: Human hearing is NOT linear - we're more sensitive to changes in lower frequencies.\n",
    "\n",
    "The **Mel scale** is a perceptual scale that:\n",
    "- Uses logarithmic spacing for frequencies\n",
    "- Approximates how humans perceive pitch\n",
    "- Maps Hz to Mel units: mel = 2595 * log10(1 + f/700)\n",
    "\n",
    "Process:\n",
    "1. Compute STFT (like regular spectrogram)\n",
    "2. Apply mel filterbank to group frequencies into mel bands\n",
    "3. Convert to log scale (log-mel spectrogram)\n",
    "\n",
    "**Very popular for speech and music ML models!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d86deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize mel spectrogram\n",
    "S_mel = librosa.feature.melspectrogram(y=audio_array, sr=sampling_rate, n_mels=128, fmax=8000)\n",
    "S_mel_db = librosa.power_to_db(S_mel, ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(S_mel_db, sr=sampling_rate, x_axis='time', y_axis='mel', fmax=8000)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mel spectrogram shape: {S_mel_db.shape}\")\n",
    "print(f\"(mel bands, time frames) = ({S_mel_db.shape[0]}, {S_mel_db.shape[1]})\")\n",
    "print(f\"\\nNotice: fewer frequency bins (128 mel bands) vs regular spectrogram ({S_db.shape[0]} bins)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aa2f36",
   "metadata": {},
   "source": [
    "### üìù Exercise 1: Compare Representations\n",
    "\n",
    "Let's compare all representations side by side!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dabe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive visualization\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# 1. Waveform\n",
    "librosa.display.waveshow(audio_array, sr=sampling_rate, ax=axes[0])\n",
    "axes[0].set_title('Time Domain: Waveform')\n",
    "axes[0].set_xlabel('Time (s)')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "\n",
    "# 2. Spectrogram\n",
    "img1 = librosa.display.specshow(S_db, sr=sampling_rate, x_axis='time', y_axis='hz', ax=axes[1])\n",
    "axes[1].set_title('Time-Frequency Domain: Spectrogram')\n",
    "fig.colorbar(img1, ax=axes[1], format='%+2.0f dB')\n",
    "\n",
    "# 3. Mel Spectrogram\n",
    "img2 = librosa.display.specshow(S_mel_db, sr=sampling_rate, x_axis='time', y_axis='mel', \n",
    "                                  fmax=8000, ax=axes[2])\n",
    "axes[2].set_title('Perceptual Representation: Mel Spectrogram')\n",
    "fig.colorbar(img2, ax=axes[2], format='%+2.0f dB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b16e5",
   "metadata": {},
   "source": [
    "### Discussion Questions\n",
    "1. What information is visible in the waveform that's hard to see in spectrograms?\n",
    "2. What can you see in spectrograms that's invisible in the waveform?\n",
    "3. How does the mel spectrogram differ from the regular spectrogram?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e75c601",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Loading and Exploring Real Datasets with ü§ó Datasets\n",
    "\n",
    "## 3.1 Introduction to Hugging Face Datasets\n",
    "\n",
    "The ü§ó Datasets library provides:\n",
    "- Easy access to thousands of audio datasets\n",
    "- Automatic downloading and caching\n",
    "- Streaming for large datasets\n",
    "- Built-in audio processing\n",
    "\n",
    "Let's load the **MINDS-14** dataset - recordings of people asking banking questions in multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b93ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MINDS-14 dataset (Australian English subset)\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")\n",
    "print(minds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc14f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a single example\n",
    "example = minds[0]\n",
    "print(\"Keys in example:\", example.keys())\n",
    "print(\"\\n--- Example Details ---\")\n",
    "print(f\"Path: {example['path']}\")\n",
    "print(f\"Transcription: {example['transcription']}\")\n",
    "print(f\"Intent class: {example['intent_class']}\")\n",
    "print(f\"\\nAudio info:\")\n",
    "print(f\"  Sampling rate: {example['audio']['sampling_rate']} Hz\")\n",
    "print(f\"  Array shape: {example['audio']['array'].shape}\")\n",
    "print(f\"  Duration: {len(example['audio']['array']) / example['audio']['sampling_rate']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert intent class to readable label\n",
    "id2label = minds.features[\"intent_class\"].int2str\n",
    "print(f\"\\nIntent label: {id2label(example['intent_class'])}\")\n",
    "\n",
    "# Listen to the audio\n",
    "print(\"\\nListen to the audio:\")\n",
    "ipd.Audio(example['audio']['array'], rate=example['audio']['sampling_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ee2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the waveform\n",
    "plt.figure(figsize=(14, 4))\n",
    "librosa.display.waveshow(example['audio']['array'], sr=example['audio']['sampling_rate'])\n",
    "plt.title(f\"Waveform: '{example['transcription']}'\")\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b87d05",
   "metadata": {},
   "source": [
    "## 3.2 Dataset Manipulation\n",
    "\n",
    "Let's learn to filter and clean datasets - removing features we don't need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6af772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "columns_to_remove = [\"english_transcription\", \"lang_id\"]\n",
    "minds_cleaned = minds.remove_columns(columns_to_remove)\n",
    "print(\"After cleaning:\")\n",
    "print(minds_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee0e4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Audio Preprocessing for Machine Learning\n",
    "\n",
    "## 4.1 Resampling Audio\n",
    "\n",
    "**Why resample?**\n",
    "- Different datasets have different sampling rates\n",
    "- ML models are trained on specific sampling rates\n",
    "- Must match the model's expected rate!\n",
    "\n",
    "Most speech models use **16 kHz** sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83068cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current sampling rate\n",
    "print(f\"Current sampling rate: {minds_cleaned[0]['audio']['sampling_rate']} Hz\")\n",
    "print(f\"Number of samples: {len(minds_cleaned[0]['audio']['array'])}\")\n",
    "\n",
    "# Resample to 16 kHz using ü§ó Datasets\n",
    "minds_resampled = minds_cleaned.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# Check after resampling\n",
    "print(f\"\\nAfter resampling:\")\n",
    "print(f\"New sampling rate: {minds_resampled[0]['audio']['sampling_rate']} Hz\")\n",
    "print(f\"Number of samples: {len(minds_resampled[0]['audio']['array'])}\")\n",
    "print(f\"\\nNote: Array length doubled (8kHz ‚Üí 16kHz upsampling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511ef2c9",
   "metadata": {},
   "source": [
    "## 4.2 Filtering by Duration\n",
    "\n",
    "Often we need to filter audio by length to:\n",
    "- Avoid memory issues (very long files)\n",
    "- Ensure consistency (remove very short clips)\n",
    "- Meet model requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add duration column\n",
    "print(\"Adding duration column...\")\n",
    "durations = []\n",
    "for x in minds_resampled: \n",
    "  durations.append(librosa.get_duration(y=x['audio']['array'], sr=x['audio']['sampling_rate']))\n",
    "minds_with_duration = minds_resampled.add_column(\"duration\", durations)\n",
    "\n",
    "print(f\"Original dataset size: {len(minds_resampled)}\")\n",
    "print(f\"Duration range: {min(durations):.2f}s - {max(durations):.2f}s\")\n",
    "\n",
    "# Filter examples shorter than 20 seconds\n",
    "MAX_DURATION = 20.0\n",
    "\n",
    "def is_audio_length_in_range(length):\n",
    "    return length < MAX_DURATION\n",
    "\n",
    "minds_filtered = minds_with_duration.filter(\n",
    "    is_audio_length_in_range, \n",
    "    input_columns=[\"duration\"]\n",
    ")\n",
    "\n",
    "# Remove temporary duration column\n",
    "minds_filtered = minds_filtered.remove_columns([\"duration\"])\n",
    "\n",
    "print(f\"Filtered dataset size: {len(minds_filtered)}\")\n",
    "print(f\"Removed {len(minds_resampled) - len(minds_filtered)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd28920",
   "metadata": {},
   "source": [
    "## 4.3 Feature Extraction for Models\n",
    "\n",
    "ML models don't directly work with raw waveforms. They need **feature extractors** to convert audio into the right format.\n",
    "\n",
    "Let's use **Whisper's feature extractor** as an example (Whisper is a state-of-the-art speech recognition model from OpenAI).\n",
    "\n",
    "**What Whisper's feature extractor does:**\n",
    "1. Pads/truncates audio to 30 seconds\n",
    "2. Converts to log-mel spectrogram (80 mel bands)\n",
    "3. No attention mask needed (unique to Whisper!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47039513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Whisper's feature extractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "print(\"Feature extractor loaded!\")\n",
    "print(f\"Expected sampling rate: {feature_extractor.sampling_rate} Hz\")\n",
    "print(f\"Mel bands (n_mels): {feature_extractor.feature_size}\")\n",
    "print(f\"FFT window size: {feature_extractor.n_fft}\")\n",
    "print(f\"Hop length: {feature_extractor.hop_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68551d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "    \n",
    "    # The feature extractor handles resampling automatically\n",
    "    features = feature_extractor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply to a single example first\n",
    "example = minds_filtered[0]\n",
    "processed = prepare_dataset(example)\n",
    "\n",
    "print(\"Processed features:\")\n",
    "print(f\"Input features shape: {np.array(processed['input_features']).shape}\")\n",
    "print(f\"(batch, mel_bands, time_frames) = {np.array(processed['input_features']).shape}\")\n",
    "print(f\"\\nThis is ready to be fed into the Whisper model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a94c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the processed features\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(\n",
    "    np.asarray(processed['input_features'][0]),\n",
    "    x_axis='time',\n",
    "    y_axis='mel',\n",
    "    sr=feature_extractor.sampling_rate,\n",
    "    hop_length=feature_extractor.hop_length\n",
    ")\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Whisper Input: Log-Mel Spectrogram (80 mel bands)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d010b051",
   "metadata": {},
   "source": [
    "### üìù Exercise 2: Process the Entire Dataset\n",
    "\n",
    "Now let's apply the preprocessing to the entire dataset using the `.map()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to entire dataset\n",
    "minds_processed = minds_filtered.map(prepare_dataset, remove_columns=[\"audio\", \"path\"])\n",
    "\n",
    "print(\"Processed dataset:\")\n",
    "print(minds_processed)\n",
    "print(f\"\\nFeatures available: {minds_processed.column_names}\")\n",
    "print(f\"\\nDataset is now ready for training or inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08d2eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Practical Exercises and Exploration\n",
    "\n",
    "## üìù Exercise 3: Explore Different Audio Representations\n",
    "\n",
    "**Task**: Load a different example audio and create all three visualizations (waveform, spectrogram, mel spectrogram) side by side.\n",
    "\n",
    "**Steps**:\n",
    "1. Try loading a different librosa example: `librosa.ex('trumpet')`, `librosa.ex('brahms')`, or `librosa.ex('choice')`\n",
    "2. Or load your own audio file using: `librosa.load('path/to/your/file.wav')`\n",
    "3. Create the three visualizations\n",
    "4. Compare and discuss what you observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Try loading a different audio example and visualizing it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b2f60",
   "metadata": {},
   "source": [
    "## üìù Exercise 4: Experiment with Different Mel Band Counts (10 minutes)\n",
    "\n",
    "**Task**: Compare mel spectrograms with different numbers of mel bands (n_mels).\n",
    "\n",
    "**Questions to explore**:\n",
    "- What happens with fewer mel bands (e.g., 40)?\n",
    "- What happens with more mel bands (e.g., 256)?\n",
    "- Which provides better frequency resolution?\n",
    "- Which is more computationally efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9961f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Compare different n_mels values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f4e36",
   "metadata": {},
   "source": [
    "## üìù Exercise 5: Explore Different Datasets (10-15 minutes)\n",
    "\n",
    "**Task**: Load a different language/dialect from MINDS-14 and compare with the Australian English version.\n",
    "\n",
    "Available languages: `en-AU`, `en-GB`, `en-US`, `de-DE`, `fr-FR`, `es-ES`, `it-IT`, `nl-NL`, `pl-PL`, `pt-PT`, `zh-CN`, `ko-KR`, etc.\n",
    "\n",
    "**Questions**:\n",
    "- How do spectrograms differ across languages?\n",
    "- Are there visible differences in speech patterns?\n",
    "- How does duration vary by language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Load a different language subset and compare\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dabddb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary and Key Takeaways\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "### 1. **Audio Representation Fundamentals**\n",
    "- **Sampling**: Converting continuous signals to discrete values\n",
    "- **Sampling Rate**: Determines maximum frequency (Nyquist limit = sr/2)\n",
    "- **Bit Depth**: Precision of amplitude values\n",
    "- Common for ML: 16 kHz sampling, 32-bit float, values in [-1.0, 1.0]\n",
    "\n",
    "### 2. **Audio Visualizations**\n",
    "- **Waveform**: Time domain - shows amplitude over time\n",
    "- **Spectrum**: Frequency domain - shows frequencies at one instant\n",
    "- **Spectrogram**: Time-frequency domain - shows how frequencies change over time\n",
    "- **Mel Spectrogram**: Perceptual scale mimicking human hearing\n",
    "\n",
    "### 3. **Working with Audio Datasets**\n",
    "- ü§ó Datasets library for easy loading and processing\n",
    "- Dataset manipulation: filtering, resampling, column management\n",
    "- Batch processing with `.map()` function\n",
    "\n",
    "### 4. **Preprocessing for ML Models**\n",
    "- **Resampling**: Match model's expected sampling rate\n",
    "- **Filtering**: Remove too-long or too-short examples\n",
    "- **Feature Extraction**: Convert raw audio to model inputs (e.g., log-mel spectrograms)\n",
    "- Different models need different preprocessing!\n",
    "\n",
    "## Important Concepts to Remember\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|-----------|\n",
    "| **Sampling Rate** | 16 kHz for speech, 44.1 kHz for music |\n",
    "| **Mel Spectrogram** | Most common input for audio ML models |\n",
    "| **Feature Extractor** | Model-specific preprocessing (always check!) |\n",
    "| **Resampling** | Must match training data sampling rate |\n",
    "| **STFT** | Short-Time Fourier Transform for spectrograms |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "This hands-on covered the **fundamentals** of audio processing. Next topics could include:\n",
    "- Audio augmentation techniques\n",
    "- Building audio classification models\n",
    "- Speech recognition with Whisper\n",
    "- Music generation models\n",
    "- Audio-to-audio tasks (source separation, enhancement)\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Additional Resources\n",
    "\n",
    "- [Hugging Face Audio Course](https://huggingface.co/learn/audio-course/chapter0/1)\n",
    "- [Librosa Documentation](https://librosa.org/doc/latest/index.html)\n",
    "- [ü§ó Datasets Documentation](https://huggingface.co/docs/datasets/)\n",
    "- [Whisper Paper](https://arxiv.org/abs/2212.04356)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

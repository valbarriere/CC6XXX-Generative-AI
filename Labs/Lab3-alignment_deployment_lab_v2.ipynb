{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "860a1885",
      "metadata": {
        "id": "860a1885"
      },
      "source": [
        "Alignment_and_Deployment_Lab_v2.ipynb\n",
        "\n",
        "# Lab: Alignment y Deployment de Modelos Generativos\n",
        "## De Raw Model a Production-Ready con SFT\n",
        "\n",
        "### Objetivos del Lab:\n",
        "1. Entender la progresión de un modelo Base a uno SFT (Supervised Fine-Tuned).\n",
        "2. Experimentar con técnicas de fine-tuning eficiente (LoRA).\n",
        "3. Comparar un modelo fine-tuned con recursos limitados contra un modelo oficial.\n",
        "\n",
        "---\n",
        "## Setup Inicial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e997f0d",
      "metadata": {
        "id": "3e997f0d"
      },
      "outputs": [],
      "source": [
        "# Instalación de dependencias\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes trl torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "943f4690",
      "metadata": {
        "id": "943f4690"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "from trl import SFTTrainer\n",
        "import pandas as pd\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66bd6dab",
      "metadata": {
        "id": "66bd6dab"
      },
      "outputs": [],
      "source": [
        "# Verificar hardware\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e6639fb",
      "metadata": {
        "id": "0e6639fb"
      },
      "source": [
        "## Utilidades Globales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "731ff5a6",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "731ff5a6"
      },
      "outputs": [],
      "source": [
        "def clear_memory():\n",
        "    \"\"\"Limpiar memoria GPU y RAM\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    print(\"Memoria limpiada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e890d1c",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "5e890d1c"
      },
      "outputs": [],
      "source": [
        "def get_gpu_memory():\n",
        "    \"\"\"Obtener uso actual de memoria GPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.memory_allocated() / 1e9\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed7b194d",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ed7b194d"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, prompt, max_length=200, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Generar texto usando el modelo\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            attention_mask=inputs.attention_mask\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text[len(prompt):].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TDicvCtjmtQo"
      },
      "id": "TDicvCtjmtQo"
    },
    {
      "cell_type": "markdown",
      "id": "a8364184",
      "metadata": {
        "id": "a8364184"
      },
      "source": [
        "# PARTE 1: El Problema - Los Modelos Base\n",
        "\n",
        "### 1.1 EJERCICIO 1: Define tus Preguntas de Evaluación\n",
        "\n",
        "**Instrucciones:**\n",
        "1. Crea una lista de 10 preguntas de evaluación en la variable `eval_questions`.\n",
        "2. Las preguntas deben ser variadas para probar diferentes capacidades (conocimiento, redacción, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55497e0c",
      "metadata": {
        "id": "55497e0c"
      },
      "outputs": [],
      "source": [
        "# TODO (EJERCICIO 1): Define tu propia lista de 10 preguntas\n",
        "eval_questions = [\n",
        "    \"¿Qué es machine learning?\",\n",
        "    # \"Pregunta 2...\",\n",
        "    # \"Pregunta 3...\",\n",
        "    # ...\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb0f399e",
      "metadata": {
        "id": "fb0f399e"
      },
      "outputs": [],
      "source": [
        "print(\"Preguntas de evaluación definidas:\")\n",
        "for i, q in enumerate(eval_questions, 1):\n",
        "    print(f\"{i}. {q}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c234df2",
      "metadata": {
        "id": "1c234df2"
      },
      "source": [
        "### 1.2 Cargar y Evaluar Modelo Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c89c734f",
      "metadata": {
        "id": "c89c734f"
      },
      "outputs": [],
      "source": [
        "model_name_base = \"Qwen/Qwen2.5-1.5B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83f8b454",
      "metadata": {
        "id": "83f8b454"
      },
      "outputs": [],
      "source": [
        "print(f\"Cargando modelo base: {model_name_base}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_base, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b7b19b",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "f3b7b19b"
      },
      "outputs": [],
      "source": [
        "model_base = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_base,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(f\"Modelo cargado. Memoria GPU usada: {get_gpu_memory():.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebf8198a",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ebf8198a"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_manual(model, tokenizer, questions, model_name=\"Model\"):\n",
        "    print(f\"\\n{'='*60}\\nEVALUACIÓN: {model_name}\\n{'='*60}\\n\")\n",
        "    results = []\n",
        "    for i, question in enumerate(questions, 1):\n",
        "        print(f\"[{i}/{len(questions)}] Pregunta: {question}\")\n",
        "        response = generate_text(model, tokenizer, question, max_length=150)\n",
        "        print(f\"Respuesta: {response[:200]}{'...' if len(response) > 200 else ''}\")\n",
        "        print(\"-\" * 60)\n",
        "        results.append({'question': question, 'response': response, 'model': model_name})\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec3ccaa9",
      "metadata": {
        "id": "ec3ccaa9"
      },
      "outputs": [],
      "source": [
        "# Guardar resultados y limpiar memoria\n",
        "results_base = evaluate_model_manual(model_base, tokenizer, eval_questions, \"BASE\")\n",
        "del model_base\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68ecc360",
      "metadata": {
        "id": "68ecc360"
      },
      "source": [
        "### 1.3 Análisis del Modelo Base\n",
        "\n",
        "**Instrucciones:**\n",
        "Basado en las respuestas guardadas en `results_base`, completa la siguiente tabla para tu análisis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo\n",
        "evaluation_ejemplo = {\n",
        "    'question': eval_questions,\n",
        "    'understood': [False, False, False, True, False, False, False, False, False, False],\n",
        "    'coherence': [2, 2, 1, 3, 1, 2, 2, 1, 2, 2],\n",
        "    'helpfulness': [1, 1, 1, 2, 1, 1, 1, 1, 1, 1],\n",
        "    'observations': [\n",
        "        'Continúa con texto técnico sin responder', 'No da explicación clara, divaga',\n",
        "        'No estructura en lista, texto confuso', 'Responde correctamente por casualidad',\n",
        "        'No sigue formato de email', 'Texto técnico incomprensible',\n",
        "        'No da receta estructurada', 'Definición vaga y confusa',\n",
        "        'Mezcla conceptos sin claridad', 'No resume, genera texto irrelevante'\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "29g7tWIEe8GX"
      },
      "id": "29g7tWIEe8GX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bb69260",
      "metadata": {
        "id": "1bb69260"
      },
      "outputs": [],
      "source": [
        "#Completar\n",
        "evaluation_base = {\n",
        "    'question': eval_questions,\n",
        "    'understood': [None] * len(eval_questions),\n",
        "    'coherence': [None] * len(eval_questions),\n",
        "    'helpfulness': [None] * len(eval_questions),\n",
        "    'observations': [''] * len(eval_questions)\n",
        "}\n",
        "df_eval_base = pd.DataFrame(evaluation_base)\n",
        "print(\"\\nTABLA DE EVALUACIÓN - MODELO BASE (Completa los valores None)\")\n",
        "print(df_eval_base.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d16a00e",
      "metadata": {
        "id": "2d16a00e"
      },
      "source": [
        "## PARTE 2: Supervised Fine-Tuning (SFT)\n",
        "\n",
        "### 2.1 Preparar Dataset y Configuración LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a8bf8af",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "3a8bf8af"
      },
      "outputs": [],
      "source": [
        "print(\"Cargando dataset Alpaca...\")\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "train_dataset = dataset.select(range(1000))\n",
        "print(f\"Usando {len(train_dataset)} ejemplos para SFT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d8dd5be",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "1d8dd5be"
      },
      "outputs": [],
      "source": [
        "def format_alpaca_instruction(sample):\n",
        "    if sample['input'] and sample['input'].strip():\n",
        "        prompt = f\"\"\"### Instruction:\\n{sample['instruction']}\\n\\n### Input:\\n{sample['input']}\\n\\n### Response:\\n{sample['output']}\"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']}\"\"\"\n",
        "    return {\"text\": prompt}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec59823",
      "metadata": {
        "id": "0ec59823"
      },
      "outputs": [],
      "source": [
        "formatted_dataset = train_dataset.map(format_alpaca_instruction)\n",
        "print(\"Dataset formateado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccb65a82",
      "metadata": {
        "id": "ccb65a82"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=1, # Probar otros valores de 1 a 32 (si tienen tiempo mas),\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05, bias=\"none\", task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "print(\"Configuración LoRA creada.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25df6f04",
      "metadata": {
        "id": "25df6f04"
      },
      "source": [
        "### 2.2 EJERCICIO 2: Experimentar con LoRA Rank\n",
        "\n",
        "**Instrucciones:**\n",
        "Modifica el valor de 'r' en la configuración LoRA y observa cómo cambia el número de parámetros entrenables. Descomenta y ejecuta el siguiente bloque.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78214da5",
      "metadata": {
        "id": "78214da5"
      },
      "outputs": [],
      "source": [
        "# TODO (EJERCICIO 2): Prueba diferentes valores de r\n",
        "def count_trainable_params(r_value):\n",
        "    lora_config_test = LoraConfig(\n",
        "        r=r_value,\n",
        "        lora_alpha=r_value * 2,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        task_type=TaskType.CAUSAL_LM)\n",
        "\n",
        "    model_temp = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name_base, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "    model_temp = get_peft_model(model_temp, lora_config_test)\n",
        "    trainable = sum(p.numel() for p in model_temp.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model_temp.parameters())\n",
        "\n",
        "    del model_temp\n",
        "    clear_memory()\n",
        "    return trainable, total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10b6a131",
      "metadata": {
        "id": "10b6a131"
      },
      "outputs": [],
      "source": [
        "r_values = [4, 8, 16, 32] # probar agregar mas valores de r\n",
        "for r in r_values:\n",
        "    trainable, total = count_trainable_params(r)\n",
        "    print(f\"r={r}: {trainable:,} params entrenables ({100*trainable/total:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4808d87",
      "metadata": {
        "id": "c4808d87"
      },
      "source": [
        "### 2.3 Entrenar y Evaluar con SFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb65d711",
      "metadata": {
        "id": "bb65d711"
      },
      "outputs": [],
      "source": [
        "print(\"Cargando modelo para SFT...\")\n",
        "model_sft = AutoModelForCausalLM.from_pretrained(model_name_base, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
        "model_sft = get_peft_model(model_sft, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f7aa50e",
      "metadata": {
        "id": "3f7aa50e"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen-sft-lora\", per_device_train_batch_size=4, gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1, learning_rate=2e-4, fp16=True, logging_steps=25, save_strategy=\"epoch\",\n",
        "    optim=\"paged_adamw_8bit\", lr_scheduler_type=\"cosine\", warmup_steps=50, report_to=\"none\", remove_unused_columns=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "873393af",
      "metadata": {
        "id": "873393af"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model_sft,\n",
        "    train_dataset=formatted_dataset,\n",
        "    args=training_args,\n",
        "    formatting_func=lambda x: x[\"text\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d26ded1",
      "metadata": {
        "id": "8d26ded1"
      },
      "outputs": [],
      "source": [
        "print(\"Iniciando entrenamiento SFT...\")\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "end_time = time.time()\n",
        "print(f\"Entrenamiento completado en {(end_time - start_time)/60:.1f} minutos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "488d8fe5",
      "metadata": {
        "id": "488d8fe5"
      },
      "outputs": [],
      "source": [
        "model_sft.save_pretrained(\"./qwen-sft-adapters\")\n",
        "tokenizer.save_pretrained(\"./qwen-sft-adapters\")\n",
        "print(\"Adapters LoRA guardados en ./qwen-sft-adapters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d03adad",
      "metadata": {
        "id": "8d03adad"
      },
      "outputs": [],
      "source": [
        "# Guardar resultados y limpiar memoria\n",
        "results_sft = evaluate_model_manual(model_sft, tokenizer, eval_questions, \"SFT\")\n",
        "del model_sft, trainer\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffeb4a7e",
      "metadata": {
        "id": "ffeb4a7e"
      },
      "source": [
        "## PARTE 3: Comparación Final\n",
        "\n",
        "Ahora, comparemos nuestro modelo SFT con el modelo oficial de Qwen.\n",
        "\n",
        "### 3.1 Cargar y Evaluar Modelo Instruct Oficial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9375fe0c",
      "metadata": {
        "id": "9375fe0c"
      },
      "outputs": [],
      "source": [
        "print(\"Cargando Qwen2.5-1.5B-Instruct (modelo oficial)...\")\n",
        "model_instruct = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"Modelo oficial cargado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fde6dd81",
      "metadata": {
        "id": "fde6dd81"
      },
      "outputs": [],
      "source": [
        "# Guardar resultados y limpiar memoria\n",
        "results_instruct = evaluate_model_manual(model_instruct, tokenizer, eval_questions, \"INSTRUCT\")\n",
        "del model_instruct\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "157c244d",
      "metadata": {
        "id": "157c244d"
      },
      "source": [
        "### 3.2 EJERCICIO 3: Comparación Triple (Base vs SFT vs Official)\n",
        "\n",
        "**Instrucciones:**\n",
        "1. Ejecuta la celda para ver las respuestas de los tres modelos lado a lado.\n",
        "2. Compara la calidad, detalle y estilo de tu modelo SFT con el modelo oficial.\n",
        "3. Reflexiona sobre qué tan cerca llegaste del modelo oficial con recursos limitados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e679e91",
      "metadata": {
        "id": "4e679e91"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"COMPARACIÓN FINAL: BASE vs NUESTRO SFT vs OFFICIAL INSTRUCT\")\n",
        "print(\"=\"*100)\n",
        "print(\"\\nRecuerda: Entrenamos con solo 1000 ejemplos SFT.\")\n",
        "print(\"El modelo oficial fue entrenado con millones de ejemplos.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9125b9",
      "metadata": {
        "id": "9c9125b9"
      },
      "outputs": [],
      "source": [
        "for i, question in enumerate(eval_questions):\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(f\"Pregunta: {question}\")\n",
        "    print(f\"{'='*100}\")\n",
        "\n",
        "    print(\"\\nMODELO BASE:\")\n",
        "    print(results_base.iloc[i]['response'])\n",
        "\n",
        "    print(\"\\nNUESTRO SFT:\")\n",
        "    print(results_sft.iloc[i]['response'])\n",
        "\n",
        "    print(\"\\nOFFICIAL INSTRUCT:\")\n",
        "    print(results_instruct.iloc[i]['response'])\n",
        "\n",
        "    print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e4b412",
      "metadata": {
        "id": "48e4b412"
      },
      "source": [
        "### 3.3 Reflexión Final\n",
        "\n",
        "**Preguntas para discutir:**\n",
        "1. ¿Qué tan cerca llegamos del modelo oficial con recursos limitados?\n",
        "2. ¿En qué aspectos nuestro modelo SFT es competitivo?\n",
        "3. ¿Dónde se nota más la diferencia en la escala de entrenamiento?\n",
        "4. ¿Qué mejorarías si tuvieras más recursos (datos, compute, tiempo)?\n",
        "\n",
        "---\n",
        "## Resumen del Lab\n",
        "\n",
        "### Lo que aprendimos:\n",
        "1. **SFT enseña el formato de \"instruction-following\" de forma eficiente con LoRA.**\n",
        "2. **La escala de datos importa, pero la técnica es accesible.** Con pocos datos se logran grandes mejoras.\n",
        "\n",
        "### Próximos Pasos:\n",
        "- Experimentar con más datos de SFT.\n",
        "- Probar con un dataset más específico a un dominio.\n",
        "- Evaluar con métricas automáticas (ROUGE, BLEU).\n",
        "\n",
        "---\n",
        "## Limpieza Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0ed23c4",
      "metadata": {
        "id": "a0ed23c4"
      },
      "outputs": [],
      "source": [
        "clear_memory()\n",
        "print(\"Lab completado!\")\n",
        "print(\"Archivos generados:\")\n",
        "print(\"  - ./qwen-sft-adapters/: Adapters LoRA después de SFT\")\n",
        "print(\"¡Excelente trabajo!\")"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "encoding": "# -*- coding: utf-8 -*-",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
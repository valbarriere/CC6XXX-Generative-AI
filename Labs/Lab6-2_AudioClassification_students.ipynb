{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f93508f",
   "metadata": {},
   "source": [
    "# Hands-on Session 6: Music Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f0839",
   "metadata": {},
   "source": [
    "# Part 1: Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082e928b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1.1 Introduction to Audio Transformers\n",
    "\n",
    "Modern audio processing heavily relies on **Transformer architectures**, originally designed for NLP but adapted brilliantly for audio tasks.\n",
    "\n",
    "### Why Transformers for Audio?\n",
    "\n",
    "Traditional approaches used CNNs on spectrograms. Transformers offer:\n",
    "- **Self-attention**: Captures long-range dependencies in audio\n",
    "- **Transfer learning**: Pre-trained on massive datasets\n",
    "- **Flexibility**: Same architecture for multiple tasks\n",
    "\n",
    "### Popular Audio Transformer Models\n",
    "\n",
    "| Model | Description | Use Cases |\n",
    "|-------|-------------|-----------|\n",
    "| **Wav2Vec2** | Self-supervised learning from raw waveforms | Speech recognition, audio classification |\n",
    "| **HuBERT** | Hidden Unit BERT - masked prediction | Speech tasks, speaker recognition |\n",
    "| **DistilHuBERT** | Distilled (smaller, faster) version of HuBERT | Resource-constrained applications |\n",
    "| **Whisper** | Multilingual speech recognition | Transcription, translation |\n",
    "| **Audio Spectrogram Transformer (AST)** | Vision Transformer applied to spectrograms | General audio classification |\n",
    "\n",
    "![Audio Transformer Architecture](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/wav2vec2.png)\n",
    "\n",
    "*Source: Hugging Face - Wav2Vec2 architecture processes raw waveforms through convolutional layers then transformer blocks*\n",
    "\n",
    "The input can also be a spectrogram, like for the AST model: \n",
    "\n",
    "![AST Architecture](https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/ast.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb237a6",
   "metadata": {},
   "source": [
    "## 1.2 How Audio Models Process Data\n",
    "\n",
    "### From Raw Audio to Model Input\n",
    "\n",
    "Let's understand the pipeline:\n",
    "\n",
    "```\n",
    "Raw Audio (waveform)\n",
    "    ‚Üì\n",
    "Feature Extraction (normalization, padding)\n",
    "    ‚Üì\n",
    "Model Encoder (self-attention layers)\n",
    "    ‚Üì\n",
    "Classification Head\n",
    "    ‚Üì\n",
    "Predictions (genre, intent, etc.)\n",
    "```\n",
    "\n",
    "### Key Concepts in Audio Transformers\n",
    "\n",
    "**1. Feature Normalization**\n",
    "- Audio samples normalized to **zero mean, unit variance**\n",
    "- Ensures stable training and convergence\n",
    "- Done by the feature extractor automatically\n",
    "\n",
    "**2. Attention Mechanism**\n",
    "- Models learn which parts of audio are important\n",
    "- Can focus on different time segments\n",
    "- Captures temporal patterns and relationships\n",
    "\n",
    "**3. Pre-training Strategy**\n",
    "- **Contrastive Learning**: Distinguish between different audio segments\n",
    "- **Masked Prediction**: Predict masked parts of audio (like BERT)\n",
    "- **Multi-task Learning**: Train on multiple related tasks\n",
    "\n",
    "### Connection to Our Task\n",
    "\n",
    "When we fine-tune HuBERT for music classification:\n",
    "- We keep the **pre-trained encoder** (learned from 60,000 hours of speech!)\n",
    "- The encoder already knows how to extract meaningful temporal patterns\n",
    "- We just add a new **classification head** and adapt to music\n",
    "- This is why transfer learning works so well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038832e4",
   "metadata": {},
   "source": [
    "## 1.3 How HuBERT Works\n",
    "\n",
    "HuBERT (Hidden Unit BERT) is a fascinating model that applies BERT-style pre-training to speech. Let's understand its clever approach!\n",
    "\n",
    "![HuBERT Overview](https://jonathanbgn.com/assets/images/illustrated-hubert/hubert_explained.png)\n",
    "\n",
    "*Source: [Jonathan Bgn - HuBERT Visually Explained](https://jonathanbgn.com/2021/10/30/hubert-visually-explained.html)*\n",
    "\n",
    "#### The Core Idea: Discovering \"Hidden Units\"\n",
    "\n",
    "The key insight: **Speech needs to be converted into discrete units (like words in text) before applying BERT!**\n",
    "\n",
    "**Problem**: Unlike text (which already has discrete words/tokens), raw audio is continuous. How do we create discrete units?\n",
    "\n",
    "**Solution**: Use clustering! \n",
    "\n",
    "#### Step 1: Clustering to Create Pseudo-Labels\n",
    "\n",
    "HuBERT uses **K-means clustering** to group similar audio segments (25ms each) into K clusters. Each cluster becomes a \"hidden unit\" - think of it as discovering the phonetic alphabet of speech automatically!\n",
    "\n",
    "![HuBERT Clustering](https://jonathanbgn.com/assets/images/illustrated-hubert/hubert_clustering.png)\n",
    "\n",
    "*Clustering process: Audio segments are grouped into K clusters, creating discrete labels*\n",
    "\n",
    "**How it works:**\n",
    "1. Take raw audio and split into 25ms segments\n",
    "2. Extract features (MFCCs for first iteration, later use learned representations)\n",
    "3. Apply K-means to assign each segment to a cluster\n",
    "4. Each cluster ID becomes a \"target label\" for that audio segment\n",
    "\n",
    "**Multi-iteration refinement:**\n",
    "- **1st iteration**: Use MFCC features for clustering (handcrafted features)\n",
    "- **2nd iteration**: Use representations from 6th transformer layer (learned features)\n",
    "- **3rd iteration** (LARGE/X-LARGE): Use 9th layer representations (even better!)\n",
    "\n",
    "Each iteration produces better \"hidden units\" as the model learns more sophisticated representations!\n",
    "\n",
    "#### Step 2: Masked Prediction (Like BERT)\n",
    "\n",
    "Once we have discrete labels from clustering, we train exactly like BERT:\n",
    "\n",
    "![HuBERT Prediction](https://jonathanbgn.com/assets/images/illustrated-hubert/hubert_pretraining_prediction.png)\n",
    "\n",
    "*Prediction step: Mask ~50% of inputs, predict their cluster assignments from context*\n",
    "\n",
    "**Training process:**\n",
    "1. **Mask**: Hide ~50% of the audio frames\n",
    "2. **Predict**: Model predicts cluster assignments for masked positions\n",
    "3. **Learn**: Use cross-entropy loss (simple and stable!)\n",
    "4. **Key trick**: Only compute loss on masked positions (handles noisy labels better)\n",
    "\n",
    "#### Why HuBERT is Powerful\n",
    "\n",
    "**Advantages over Wav2Vec2:**\n",
    "- **Simpler loss**: Cross-entropy vs contrastive + diversity loss\n",
    "- **More stable training**: No temperature tuning needed\n",
    "- **Better targets**: Re-uses learned representations for clustering\n",
    "- **Matches or beats Wav2Vec2** on speech recognition!\n",
    "\n",
    "**The iterative magic:**\n",
    "```\n",
    "Iteration 1: MFCC ‚Üí Clustering ‚Üí Train ‚Üí Get basic representations\n",
    "                              ‚Üì\n",
    "Iteration 2: Layer 6 features ‚Üí Better clustering ‚Üí Train ‚Üí Get better representations  \n",
    "                              ‚Üì\n",
    "Iteration 3: Layer 9 features ‚Üí Even better clustering ‚Üí Train ‚Üí Best representations!\n",
    "```\n",
    "\n",
    "Each iteration discovers more meaningful \"hidden units\" - from crude phonetic categories to fine-grained acoustic patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c6c727",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Hands-on: Fine-tuning for Music Genre Classification\n",
    "\n",
    "Now let's put everything into practice! We'll fine-tune a pre-trained audio model to classify music genres.\n",
    "\n",
    "## 2.1 The GTZAN Dataset\n",
    "\n",
    "**GTZAN** is a famous music classification dataset:\n",
    "- **1,000 songs** (30 seconds each)\n",
    "- **10 genres**: Blues, Classical, Country, Disco, Hip-hop, Jazz, Metal, Pop, Reggae, Rock\n",
    "- **Balanced dataset**: 100 songs per genre\n",
    "\n",
    "This is a challenging task - even humans sometimes disagree on genre classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcae926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GTZAN dataset\n",
    "gtzan = load_dataset(\"marsyas/gtzan\", \"all\")\n",
    "print(\"Original dataset:\")\n",
    "print(gtzan)\n",
    "\n",
    "# Note: One recording is corrupted, so we have 999 instead of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df813601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split (90/10)\n",
    "gtzan = gtzan[\"train\"].train_test_split(seed=42, shuffle=True, test_size=0.1)\n",
    "print(\"\\nAfter split:\")\n",
    "print(gtzan)\n",
    "print(f\"\\nTraining samples: {len(gtzan['train'])}\")\n",
    "print(f\"Test samples: {len(gtzan['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b4f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a sample\n",
    "sample = gtzan[\"train\"][0]\n",
    "print(\"Sample structure:\")\n",
    "print(f\"Keys: {sample.keys()}\")\n",
    "print(f\"\\nGenre (as integer): {sample['genre']}\")\n",
    "print(f\"Sampling rate: {sample['audio']['sampling_rate']} Hz\")\n",
    "print(f\"Duration: {len(sample['audio']['array']) / sample['audio']['sampling_rate']:.2f} seconds\")\n",
    "\n",
    "# Get genre label\n",
    "id2label_fn = gtzan[\"train\"].features[\"genre\"].int2str\n",
    "print(f\"Genre (as label): {id2label_fn(sample['genre'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adafaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to a few examples from different genres\n",
    "print(\"Listen to different music genres:\\n\")\n",
    "\n",
    "for _ in range(3):\n",
    "    example = gtzan[\"train\"].shuffle()[0]\n",
    "    genre_label = id2label_fn(example[\"genre\"])\n",
    "    print(f\"Genre: {genre_label}\")\n",
    "    display(ipd.Audio(example[\"audio\"][\"array\"], rate=example[\"audio\"][\"sampling_rate\"]))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf45f51",
   "metadata": {},
   "source": [
    "## 2.2 Choosing a Pre-trained Model: DistilHuBERT\n",
    "\n",
    "For this task, we'll use **DistilHuBERT**:\n",
    "- **Distilled version** of HuBERT (73% faster!)\n",
    "- Pre-trained on LibriSpeech (speech data)\n",
    "- **Transfer learning**: We adapt it from speech ‚Üí music\n",
    "- Compact enough for Google Colab free tier\n",
    "\n",
    "### Model Architecture Overview\n",
    "\n",
    "```\n",
    "Input: Raw Waveform (16kHz)\n",
    "    ‚Üì\n",
    "CNN Feature Extractor (7 conv layers)\n",
    "    ‚Üì\n",
    "Transformer Encoder (12 layers, 768 hidden dim)\n",
    "    ‚Üì\n",
    "Classification Head (768 ‚Üí C classes)\n",
    "    ‚Üì\n",
    "Output: Genre Predictions\n",
    "```\n",
    "\n",
    "![Fine-tuning Process](https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/wav2vec2-ctc.png)\n",
    "\n",
    "*Fine-tuning: We freeze or update the pre-trained encoder and train a new classification head (not CTC)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1572f139",
   "metadata": {},
   "source": [
    "## 2.3 Preprocessing for DistilHuBERT\n",
    "\n",
    "Key preprocessing steps:\n",
    "1. **Resample** to 16 kHz (model's expected rate)\n",
    "2. **Normalize** audio (zero mean, unit variance)\n",
    "3. **Truncate** to maximum length (30 seconds)\n",
    "4. Generate **attention masks** for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature extractor for DistilHuBERT\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "model_id = \"ntu-spml/distilhubert\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_id, \n",
    "    do_normalize=True, \n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "print(f\"Feature extractor loaded!\")\n",
    "print(f\"Expected sampling rate: {feature_extractor.sampling_rate} Hz\")\n",
    "print(f\"Normalization: {feature_extractor.do_normalize}\")\n",
    "print(f\"Return attention mask: {feature_extractor.return_attention_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the dataset to 16kHz\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "gtzan = gtzan.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "\n",
    "print(f\"Dataset resampled to {sampling_rate} Hz\")\n",
    "print(f\"Example sampling rate: {gtzan['train'][0]['audio']['sampling_rate']} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c6cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand feature normalization with an example\n",
    "sample = gtzan[\"train\"][0][\"audio\"]\n",
    "\n",
    "print(\"Before feature extraction:\")\n",
    "print(f\"Mean: {np.mean(sample['array']):.6f}\")\n",
    "print(f\"Variance: {np.var(sample['array']):.6f}\")\n",
    "print(f\"Range: [{sample['array'].min():.3f}, {sample['array'].max():.3f}]\")\n",
    "\n",
    "# Apply feature extractor\n",
    "inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
    "\n",
    "print(\"\\nAfter feature extraction:\")\n",
    "print(f\"Keys: {list(inputs.keys())}\")\n",
    "print(f\"Mean: {np.mean(inputs['input_values']):.9f}\")\n",
    "print(f\"Variance: {np.var(inputs['input_values']):.6f}\")\n",
    "print(f\"Range: [{np.min(inputs['input_values']):.3f}, {np.max(inputs['input_values']):.3f}]\")\n",
    "print(\"\\nAudio normalized to zero mean and unit variance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa29de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function for the entire dataset\n",
    "max_duration = 30.0\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess audio examples for DistilHuBERT\"\"\"\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration),\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "print(\"Preprocessing dataset... (this may take a few minutes)\")\n",
    "gtzan_encoded = gtzan.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=[\"audio\", \"file\"],\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "print(\"\\nPreprocessing complete!\")\n",
    "print(gtzan_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'genre' to 'label' for the Trainer\n",
    "gtzan_encoded = gtzan_encoded.rename_column(\"genre\", \"label\")\n",
    "\n",
    "# Create label mappings\n",
    "id2label = {\n",
    "    str(i): id2label_fn(i) \n",
    "    for i in range(len(gtzan_encoded[\"train\"].features[\"label\"].names))\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(\"Label mappings:\")\n",
    "for i in range(len(id2label)):\n",
    "    print(f\"{i}: {id2label[str(i)]}\")\n",
    "\n",
    "print(f\"\\nDataset ready for training!\")\n",
    "print(f\"Features: {gtzan_encoded['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f4916",
   "metadata": {},
   "source": [
    "## 2.4 Loading the Model for Fine-tuning\n",
    "\n",
    "Now we load the pre-trained DistilHuBERT model and add a classification head on top for our 10 genres.\n",
    "\n",
    "### What Happens During Loading:\n",
    "1. **Download pre-trained weights** from Hugging Face Hub\n",
    "2. **Remove the original head** (designed for speech tasks)\n",
    "3. **Add new classification head** (768 hidden ‚Üí 10 genres)\n",
    "4. **Initialize new head** with random weights\n",
    "5. Keep pre-trained encoder weights (transfer learning!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with classification head\n",
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "num_labels = len(id2label)\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Model: {model_id}\")\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(f\"Total parameters: {model.num_parameters():,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d4b77",
   "metadata": {},
   "source": [
    "## 2.5 Setting Up Training\n",
    "\n",
    "We'll use the ü§ó Transformers `Trainer` - a high-level API that handles:\n",
    "- Training loop\n",
    "- Gradient computation and optimization\n",
    "- Evaluation\n",
    "- Logging and checkpointing\n",
    "- Mixed precision training (FP16)\n",
    "- Automatic model uploading to Hub\n",
    "\n",
    "### Training Hyperparameters\n",
    "\n",
    "Key hyperparameters to consider:\n",
    "- **Learning rate**: 5e-5 (standard for fine-tuning)\n",
    "- **Batch size**: 8 (adjust based on GPU memory)\n",
    "- **Epochs**: 10 (balance between training time and performance)\n",
    "- **Warmup**: 10% of steps (gradual learning rate increase)\n",
    "- **Evaluation strategy**: Every epoch\n",
    "- **FP16**: Mixed precision for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 10\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-gtzan\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,  # Mixed precision training\n",
    "    push_to_hub=False,  # Set to True if you want to push to Hub\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")\n",
    "print(f\"Output directory: {training_args.output_dir}\")\n",
    "print(f\"Training epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e8425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metric\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "print(\"Evaluation metric (accuracy) loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da846e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=gtzan_encoded[\"train\"],\n",
    "    eval_dataset=gtzan_encoded[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")\n",
    "print(f\"Training samples: {len(gtzan_encoded['train'])}\")\n",
    "print(f\"Evaluation samples: {len(gtzan_encoded['test'])}\")\n",
    "print(f\"Steps per epoch: {len(gtzan_encoded['train']) // batch_size}\")\n",
    "print(f\"Total training steps: {(len(gtzan_encoded['train']) // batch_size) * num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c4f77",
   "metadata": {},
   "source": [
    "## 2.6 Training the Model\n",
    "\n",
    "‚ö†Ô∏è **Note**: Full training takes ~1 hour on a T4 GPU. For the hands-on session, you can:\n",
    "- Reduce `num_train_epochs` to 2-3 for faster demo\n",
    "- Use a pre-trained checkpoint (see next section)\n",
    "- Discuss the training process while showing pre-computed results\n",
    "\n",
    "### What Happens During Training:\n",
    "\n",
    "1. **Forward Pass**: Audio ‚Üí CNN ‚Üí Transformer ‚Üí Classification head ‚Üí Predictions\n",
    "2. **Loss Computation**: Compare predictions with true labels (Cross-Entropy)\n",
    "3. **Backward Pass**: Compute gradients via backpropagation\n",
    "4. **Optimizer Step**: Update weights (AdamW optimizer)\n",
    "5. **Evaluation**: After each epoch, evaluate on test set\n",
    "6. **Checkpointing**: Save best model based on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7833a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# UNCOMMENT THE LINE BELOW TO START TRAINING\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration purposes, we'll show expected training results:\n",
    "print(\"Expected Training Results (10 epochs):\")\n",
    "print(\"=\" * 60)\n",
    "print(\"| Epoch | Train Loss | Val Loss | Accuracy |\")\n",
    "print(\"|-------|------------|----------|----------|\")\n",
    "print(\"|   1   |   1.73     |   1.80   |   0.44   |\")\n",
    "print(\"|   2   |   1.24     |   1.30   |   0.64   |\")\n",
    "print(\"|   3   |   0.98     |   0.99   |   0.70   |\")\n",
    "print(\"|   4   |   0.69     |   0.75   |   0.79   |\")\n",
    "print(\"|   5   |   0.45     |   0.62   |   0.81   |\")\n",
    "print(\"|   6   |   0.30     |   0.54   |   0.83   | <- Best\")\n",
    "print(\"|   7   |   0.22     |   0.63   |   0.78   |\")\n",
    "print(\"|   8   |   0.31     |   0.59   |   0.81   |\")\n",
    "print(\"|   9   |   0.16     |   0.54   |   0.83   |\")\n",
    "print(\"|  10   |   0.12     |   0.57   |   0.82   |\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nBest validation accuracy: 83%\")\n",
    "print(\"Training time: ~60 minutes on T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9061c1",
   "metadata": {},
   "source": [
    "## 2.7 Using a Pre-trained Model for Inference\n",
    "\n",
    "Instead of training from scratch, we can use an already fine-tuned model from the Hub!\n",
    "\n",
    "### Using the Pipeline API\n",
    "\n",
    "The simplest way to use a model is through the `pipeline()` API - a high-level interface for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff9f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained music classification model from the Hub\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use a fine-tuned model (this is the model from the HF course)\n",
    "pipe = pipeline(\n",
    "    \"audio-classification\",\n",
    "    model=\"sanchit-gandhi/distilhubert-finetuned-gtzan\"\n",
    ")\n",
    "\n",
    "print(\"Pre-trained model loaded!\")\n",
    "print(f\"Model: {pipe.model.name_or_path}\")\n",
    "print(f\"Task: {pipe.task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on samples from our dataset\n",
    "print(\"Testing the model on GTZAN samples:\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    # Get a random sample\n",
    "    example = gtzan[\"test\"].shuffle()[i]\n",
    "    true_genre = id2label_fn(example[\"genre\"])\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = pipe(example[\"audio\"][\"array\"])\n",
    "    \n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  True genre: {true_genre}\")\n",
    "    print(f\"  Top 3 predictions:\")\n",
    "    for pred in predictions[:3]:\n",
    "        print(f\"    - {pred['label']}: {pred['score']:.2%}\")\n",
    "    print()\n",
    "    \n",
    "    # Play the audio\n",
    "    display(ipd.Audio(example[\"audio\"][\"array\"], rate=example[\"audio\"][\"sampling_rate\"]))\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738eff1f",
   "metadata": {},
   "source": [
    "## 2.8 Model Analysis and Interpretability\n",
    "\n",
    "Let's analyze what the model learned and where it makes mistakes.\n",
    "\n",
    "### Understanding Model Performance\n",
    "\n",
    "**Confusion Matrix**: Shows which genres get confused with each other\n",
    "**Common confusions**:\n",
    "- Rock ‚Üî Metal (similar instrumentation)\n",
    "- Jazz ‚Üî Blues (overlapping styles)\n",
    "- Disco ‚Üî Pop (similar production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d464162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions on the entire test set\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Making predictions on test set...\")\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for example in gtzan[\"test\"]:\n",
    "    pred = pipe(example[\"audio\"][\"array\"])\n",
    "    predicted_label = pred[0][\"label\"]\n",
    "    true_label = id2label_fn(example[\"genre\"])\n",
    "    \n",
    "    all_predictions.append(predicted_label)\n",
    "    all_labels.append(true_label)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(all_labels, all_predictions, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "genres = sorted(set(all_labels))\n",
    "cm = confusion_matrix(all_labels, all_predictions, labels=genres)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=genres, yticklabels=genres)\n",
    "plt.title('Music Genre Classification - Confusion Matrix', fontsize=14, pad=20)\n",
    "plt.xlabel('Predicted Genre', fontsize=12)\n",
    "plt.ylabel('True Genre', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b62e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practical Exercises\n",
    "\n",
    "### Exercise 1: Error Analysis\n",
    "\n",
    "**Goal**: Find and analyze misclassified examples to understand model limitations.\n",
    "\n",
    "**Tasks**:\n",
    "- Find 5 misclassified examples from the test set\n",
    "- Listen to them - do you agree with the model or the label?\n",
    "- What makes classification difficult for these examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Error Analysis\n",
    "# Find misclassified examples and listen to them\n",
    "\n",
    "# TODO: Create an empty list to store misclassified examples\n",
    "\n",
    "# TODO: Loop through all_predictions and all_labels using enumerate\n",
    "# Hint: for i, (pred, true) in enumerate(zip(all_predictions, all_labels)):\n",
    "    # TODO: Check if pred != true\n",
    "        # TODO: Append (i, pred, true) to the misclassified list\n",
    "        # TODO: Break when you have 5 misclassified examples\n",
    "\n",
    "print(\"Misclassified Examples:\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TODO: Loop through the misclassified list\n",
    "    # TODO: Get the example from gtzan[\"test\"] using the index\n",
    "    # TODO: Print the example number, true genre, and predicted genre\n",
    "    # TODO: Display the audio using ipd.Audio\n",
    "    # TODO: Print a separator line\n",
    "\n",
    "# Reflection questions\n",
    "print(\"\\nReflection Questions:\")\n",
    "print(\"1. Do you agree with the true labels or the model's predictions?\")\n",
    "print(\"2. What audio characteristics might have confused the model?\")\n",
    "print(\"3. Are there genres that sound similar to each other?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ff495",
   "metadata": {},
   "source": [
    "### Exercise 2: Model Comparison\n",
    "\n",
    "**Goal**: Compare different pre-trained models to see which performs best.\n",
    "\n",
    "**Tasks**:\n",
    "- Try a different model from the Hub (e.g., \"facebook/wav2vec2-base-gtzan\")\n",
    "- Make predictions on the same test samples\n",
    "- Compare predictions between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae68eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Model Comparison\n",
    "# Try a different model and compare predictions\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# TODO: Load an alternative model using pipeline\n",
    "# Some options:\n",
    "# - \"facebook/wav2vec2-base-gtzan\"\n",
    "# - \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "# Uncomment and modify to use a real alternative model:\n",
    "# pipe2 = pipeline(\"audio-classification\", model=\"MODEL_NAME_HERE\")\n",
    "\n",
    "print(\"Comparing Model Predictions:\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TODO: Test on 3 random examples\n",
    "# Hint: for i in range(3):\n",
    "    # TODO: Get a random sample from gtzan[\"test\"].shuffle()\n",
    "    # TODO: Get the true genre label using id2label_fn\n",
    "    \n",
    "    # TODO: Make predictions from both models (pipe and pipe2)\n",
    "    \n",
    "    # TODO: Print the sample number, true genre, and top predictions from both models\n",
    "    # TODO: Check if the models agree or disagree\n",
    "    # TODO: Display the audio\n",
    "    # TODO: Print a separator line\n",
    "\n",
    "print(\"\\nTry loading different models from the Hub and see which performs best!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ffed0",
   "metadata": {},
   "source": [
    "### Exercise 3: Feature Visualization\n",
    "\n",
    "**Goal**: Visualize spectrograms of correctly vs incorrectly classified songs.\n",
    "\n",
    "**Tasks**:\n",
    "- Find one correctly classified and one misclassified example\n",
    "- Visualize their mel spectrograms side-by-side\n",
    "- Look for visual differences that might explain the model's behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Feature Visualization\n",
    "# Compare spectrograms of correct vs incorrect predictions\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# TODO: Find one correct and one incorrect prediction\n",
    "# Hint: Initialize correct_idx = None and incorrect_idx = None\n",
    "# TODO: Loop through all_predictions and all_labels using enumerate\n",
    "    # TODO: If pred == true and correct_idx is None, save the index\n",
    "    # TODO: If pred != true and incorrect_idx is None, save the index\n",
    "    # TODO: Break when you have both indices\n",
    "\n",
    "# Visualize spectrograms\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# TODO: For the correct prediction:\n",
    "    # TODO: Get the example from gtzan[\"test\"]\n",
    "    # TODO: Get the true label\n",
    "    # TODO: Compute mel spectrogram using librosa.feature.melspectrogram\n",
    "    # TODO: Convert to dB using librosa.power_to_db\n",
    "    # TODO: Display using librosa.display.specshow on axes[0]\n",
    "    # TODO: Set title to show it's correctly classified\n",
    "\n",
    "# TODO: For the incorrect prediction:\n",
    "    # TODO: Get the example from gtzan[\"test\"]\n",
    "    # TODO: Get the true label and predicted label\n",
    "    # TODO: Compute mel spectrogram\n",
    "    # TODO: Convert to dB\n",
    "    # TODO: Display on axes[1]\n",
    "    # TODO: Set title to show true vs predicted labels\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef1104",
   "metadata": {},
   "source": [
    "#### Analysis Questions\n",
    "\n",
    "1. Do you see visual differences in frequency patterns?\n",
    "2. Are there specific frequency ranges that look similar/different?\n",
    "3. How does the temporal structure (over time) differ?\n",
    "4. What acoustic features might explain the misclassification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91c761d",
   "metadata": {},
   "source": [
    "### Exercise 4: Test Your Own Music\n",
    "\n",
    "**Goal**: Upload your own music file and see how the model classifies it!\n",
    "\n",
    "**Tasks**:\n",
    "- Upload a music file (or use `librosa.ex()` examples)\n",
    "- Run classification and see the top predictions\n",
    "- Discuss whether the predictions make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfa420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Test Your Own Music\n",
    "# Load and classify your own audio file\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Option 1: Use librosa example files\n",
    "# You can try: 'brahms', 'choice', 'fishin', 'nutcracker', 'trumpet'\n",
    "audio_path = librosa.ex('brahms')  # Classical music example\n",
    "\n",
    "# Option 2: Upload your own file (uncomment and modify path)\n",
    "# audio_path = \"path/to/your/music/file.mp3\"\n",
    "\n",
    "# TODO: Load the audio using librosa.load\n",
    "# Hint: audio_array, sr = librosa.load(audio_path, sr=16000, duration=30.0)\n",
    "\n",
    "print(\"Analyzing Your Music...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TODO: Make prediction using pipe\n",
    "\n",
    "# TODO: Print the top 5 genre predictions\n",
    "# Hint: Loop through predictions[:5] and print label and score\n",
    "# Optional: Create a bar visualization using \"‚ñà\" * int(pred['score'] * 50)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# TODO: Display the audio using ipd.Audio\n",
    "\n",
    "# TODO: Visualize the mel spectrogram\n",
    "# Hint: Use librosa.feature.melspectrogram and librosa.power_to_db\n",
    "# Then display with librosa.display.specshow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b2ac7",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "1. Does the top prediction match your expectation?\n",
    "2. Are there any surprising predictions in the top 5?\n",
    "3. What visual features in the spectrogram might support the prediction?\n",
    "4. Try different audio files and see how predictions change!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e581ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise Summary\n",
    "\n",
    "Great work! You've now:\n",
    "- **Exercise 1**: Analyzed model errors and found patterns\n",
    "- **Exercise 2**: Compared different models\n",
    "- **Exercise 3**: Visualized spectrograms to understand features\n",
    "- **Exercise 4**: Tested the model on your own music\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Models make mistakes on ambiguous or genre-blending songs\n",
    "- Different models can have different strengths and weaknesses\n",
    "- Visual features (spectrograms) reveal why models make certain predictions\n",
    "- Real-world music often doesn't fit neatly into single genres!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3c003",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Final Summary: Deep Learning for Audio\n",
    "\n",
    "## What We Learned About Deep Learning\n",
    "\n",
    "### 1. **Transfer Learning**\n",
    "- Pre-trained models (HuBERT, Wav2Vec2) learn general audio representations\n",
    "- Fine-tuning adapts them to specific tasks (speech to music)\n",
    "- Much faster and better than training from scratch!\n",
    "\n",
    "### 2. **Model Architecture**\n",
    "```\n",
    "Raw Audio ‚Üí CNN Feature Extractor ‚Üí Transformer Encoder ‚Üí Task Head ‚Üí Output\n",
    "```\n",
    "- **CNNs**: Extract local patterns from waveforms\n",
    "- **Transformers**: Capture long-range dependencies via self-attention\n",
    "- **Classification Head**: Task-specific layer (e.g., genre prediction)\n",
    "\n",
    "### 3. **Training Process**\n",
    "1. Load pre-trained weights (transfer learning)\n",
    "2. Add task-specific head\n",
    "3. Fine-tune on target dataset\n",
    "4. Evaluate and iterate\n",
    "\n",
    "### 4. **Key Insights**\n",
    "\n",
    "| Aspect | Key Point |\n",
    "|--------|-----------|\n",
    "| **Data** | Quality > Quantity (899 samples achieved 83% accuracy!) |\n",
    "| **Preprocessing** | Normalization crucial for stable training |\n",
    "| **Architecture** | Transformers excel at capturing temporal patterns |\n",
    "| **Evaluation** | Balanced dataset means accuracy is meaningful |\n",
    "| **Transfer Learning** | Speech models adapt well to music! |\n",
    "\n",
    "## Advanced Topics to Explore\n",
    "\n",
    "- **Data Augmentation**: Time stretching, pitch shifting, noise addition\n",
    "- **Multi-label Classification**: Songs with multiple genres\n",
    "- **Zero-shot Classification**: Using CLAP or other contrastive models\n",
    "- **Attention Visualization**: Understanding what the model focuses on\n",
    "- **Model Compression**: Quantization, pruning, distillation\n",
    "- **Real-time Inference**: Optimizing for production deployment\n",
    "\n",
    "## Key Differences: Audio vs Other Domains\n",
    "\n",
    "| Aspect | Audio | Vision | NLP |\n",
    "|--------|-------|--------|-----|\n",
    "| **Input** | 1D waveform or 2D spectrogram | 2D image | 1D token sequence |\n",
    "| **Key Challenge** | Temporal dynamics | Spatial patterns | Sequential dependencies |\n",
    "| **Pre-training** | Contrastive/Masked prediction | Image classification | Masked language modeling |\n",
    "| **Data Rate** | 16,000 samples/sec | Fixed resolution | Variable length |\n",
    "\n",
    "## Resources for Further Learning\n",
    "\n",
    "- [Hugging Face Audio Course](https://huggingface.co/learn/audio-course) - Complete course on audio ML\n",
    "- [Papers with Code - Audio Classification](https://paperswithcode.com/task/audio-classification) - Latest research\n",
    "- [Hugging Face Hub - Audio Models](https://huggingface.co/models?pipeline_tag=audio-classification) - Pre-trained models\n",
    "- [ESC-50 Dataset](https://github.com/karolpiczak/ESC-50) - Environmental sound classification\n",
    "- [AudioSet](https://research.google.com/audioset/) - Large-scale audio dataset\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed a comprehensive hands-on session covering:\n",
    "- Audio data fundamentals (sampling, representations)\n",
    "- Visualization techniques (waveform, spectrogram, mel spectrogram)\n",
    "- Dataset loading and preprocessing\n",
    "- Feature extraction for ML models\n",
    "- Deep learning with transformers\n",
    "- Fine-tuning for music classification\n",
    "- Model evaluation and analysis\n",
    "\n",
    "**You now have the tools to work with audio in your own ML projects!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
